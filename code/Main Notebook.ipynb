{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "6MmtJrhlOCdh",
        "X0FVzPEwOJa5"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NWfhMUhg_0p_"
      },
      "outputs": [],
      "source": [
        "!gdown --fuzzy https://drive.google.com/file/d/1vfqzgDFMZyn1mHlzFx-t1_KuAiNgC64f/view -O assignment_data.zip --quiet"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip assignment_data.zip"
      ],
      "metadata": {
        "id": "r2koqFqfBTzv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/CS512_Assignment"
      ],
      "metadata": {
        "id": "youaau8bUehR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e4504d1-9b2c-4d3a-9a13-5dd1167460e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/CS512_Assignment\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!git clone --quiet https://github.com/shangjingbo1226/AutoPhrase\n",
        "#!git clone --quiet https://github.com/yumeng5/CatE\n",
        "\n",
        "#%cd /content/drive/MyDrive/CS512_Assignment/assignment\n",
        "#!cat movies_train.txt movies_test.txt > movies_combined.txt\n",
        "#!cat news_train.txt news_test.txt > news_combined.txt\n",
        "\n",
        "#!mv movies_combined.txt ../AutoPhrase/data/EN/\n",
        "#!mv news_combined.txt ../AutoPhrase/data/EN/"
      ],
      "metadata": {
        "id": "8hxZFW44Ct5T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd\n",
        "!ls -l auto_phrase.sh\n",
        "!ls -l ./bin/segphrase_train\n",
        "!ls -l ./tools/treetagger/pos_tag.sh\n",
        "\n",
        "!chmod 755 auto_phrase.sh\n",
        "!chmod 755 ./bin/segphrase_train\n",
        "!chmod 755 ./tools/treetagger/pos_tag.sh\n",
        "\n",
        "!chmod 755 ./bin/segphrase_segment\n",
        "\n",
        "!ls -l auto_phrase.sh\n",
        "!ls -l ./bin/segphrase_train\n",
        "!ls -l ./tools/treetagger/pos_tag.sh\n",
        "!ls -l ./bin/segphrase_segment"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3_Iarqixpq7W",
        "outputId": "38f420ab-df83-472a-d1fa-a5afcba58feb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/CS512_Assignment/AutoPhrase\n",
            "-rwx------ 1 root root 7255 Oct 23 21:12 auto_phrase.sh\n",
            "-rwx------ 1 root root 241856 Oct 10 01:06 ./bin/segphrase_train\n",
            "-rwx------ 1 root root 1585 Oct 10 00:45 ./tools/treetagger/pos_tag.sh\n",
            "-rwx------ 1 root root 7255 Oct 23 21:12 auto_phrase.sh\n",
            "-rwx------ 1 root root 241856 Oct 10 01:06 ./bin/segphrase_train\n",
            "-rwx------ 1 root root 1585 Oct 10 00:45 ./tools/treetagger/pos_tag.sh\n",
            "-rwx------ 1 root root 181896 Oct 10 01:06 ./bin/segphrase_segment\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Run Auto Phrase"
      ],
      "metadata": {
        "id": "6MmtJrhlOCdh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/CS512_Assignment/AutoPhrase\n",
        "!bash ./auto_phrase.sh"
      ],
      "metadata": {
        "id": "M1Ks_0mnC8g6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "93b0a6d1-079b-4009-9ce5-2bbf6134240f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/CS512_Assignment/AutoPhrase\n",
            "\u001b[32m===Compilation===\u001b[m\n",
            "\u001b[32m===Tokenization===\u001b[m\n",
            "\n",
            "real\t0m20.291s\n",
            "user\t0m31.263s\n",
            "sys\t0m2.146s\n",
            "Detected Language: EN\u001b[0K\n",
            "Current step: Tokenizing wikipedia phrases...\u001b[0K\n",
            "No provided expert labels.\u001b[0K\n",
            "\u001b[32m===AutoPhrasing===\u001b[m\n",
            "=== Current Settings ===\n",
            "Iterations = 2\n",
            "Minimum Support Threshold = 10\n",
            "Maximum Length Threshold = 6\n",
            "POS-Tagging Mode Disabled\n",
            "Discard Ratio = 0.050000\n",
            "Number of threads = 10\n",
            "Labeling Method = DPDN\n",
            "\tAuto labels from knowledge bases\n",
            "\tMax Positive Samples = -1\n",
            "=======\n",
            "Loading data...\n",
            "# of total tokens = 5631756\n",
            "max word token id = 71763\n",
            "# of documents = 120000\n",
            "# of distinct POS tags = 0\n",
            "Mining frequent phrases...\n",
            "selected MAGIC = 71777\n",
            "# of frequent phrases = 166834\n",
            "Extracting features...\n",
            "Constructing label pools...\n",
            "\tThe size of the positive pool = 15020\n",
            "\tThe size of the negative pool = 150655\n",
            "# truth patterns = 158486\n",
            "Estimating Phrase Quality...\n",
            "Segmenting...\n",
            "Rectifying features...\n",
            "Estimating Phrase Quality...\n",
            "Segmenting...\n",
            "Dumping results...\n",
            "Done.\n",
            "\n",
            "real\t2m10.202s\n",
            "user\t3m1.173s\n",
            "sys\t0m15.775s\n",
            "\u001b[32m===Saving Model and Results===\u001b[m\n",
            "\u001b[32m===Generating Output===\u001b[m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Perform phrasal segmentation using phrases extracted from autophrase"
      ],
      "metadata": {
        "id": "X0FVzPEwOJa5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/CS512_Assignment/AutoPhrase\n",
        "!bash ./phrasal_segmentation.sh"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DBml_IBGaiQo",
        "outputId": "f3fefcda-9ac5-4e11-a264-55ad8c090813"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/CS512_Assignment/AutoPhrase\n",
            "\u001b[32m===Compilation===\u001b[m\n",
            "\u001b[32m===Tokenization===\u001b[m\n",
            "\n",
            "real\t0m11.975s\n",
            "user\t0m14.499s\n",
            "sys\t0m1.061s\n",
            "Detected Language: EN\u001b[0K\n",
            "\u001b[32m===Phrasal Segmentation===\u001b[m\n",
            "=== Current Settings ===\n",
            "Segmentation Model Path = models/DBLP/segmentation_news.model\n",
            "After the phrasal segmentation, only following phrases will be highlighted with <phrase> and </phrase>\n",
            "\tQ(multi-word phrases) >= 0.700000\n",
            "\tQ(single-word phrases) >= 1.000000\n",
            "=======\n",
            "Length penalty model loaded.\n",
            "\tpenalty = 199.805\n",
            "# of loaded patterns = 39258\n",
            "# of loaded truth patterns = 173506\n",
            "Phrasal segmentation finished.\n",
            "   # of total highlighted quality phrases = 999573\n",
            "   # of total processed sentences = 738494\n",
            "   avg highlights per sentence = 1.35353\n",
            "\n",
            "real\t0m10.858s\n",
            "user\t0m9.578s\n",
            "sys\t0m0.219s\n",
            "\u001b[32m===Generating Output===\u001b[m\n",
            "\n",
            "real\t0m8.004s\n",
            "user\t0m8.258s\n",
            "sys\t0m0.532s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Run CatE"
      ],
      "metadata": {
        "id": "TVxLzfFwOTVH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import os\n",
        "import csv\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk import word_tokenize\n",
        "import re"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "puI6pWxYqGxs",
        "outputId": "4f32d349-1c0c-438e-f0bc-7d38d98e325e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def phrase_process(dataset):\n",
        "  f = open(os.path.join('/content/drive/MyDrive/CS512_Assignment/CatE/datasets', dataset, f'segmentation_{dataset}.txt'))\n",
        "  out_file = f'/content/drive/MyDrive/CS512_Assignment/CatE/datasets/{dataset}/segmentation_{dataset}_phrase_processed.txt'\n",
        "  g = open(out_file, 'w')\n",
        "  for line in tqdm(f):\n",
        "    doc = ''\n",
        "    temp = re.split(r'<phrase_Q=\\d\\.\\d+>', line) #temp = line.split('<phrase>')\n",
        "    for seg in temp:\n",
        "      temp2 = seg.split('</phrase>')\n",
        "      if len(temp2) > 1:\n",
        "        doc += ('_').join(temp2[0].split(' ')) + temp2[1]\n",
        "      else:\n",
        "        doc += temp2[0]\n",
        "    g.write(doc.strip()+'\\n')\n",
        "  print(\"Phrase segmented corpus written to {}\".format(out_file))\n",
        "  return\n",
        "\n",
        "\n",
        "def preprocess(dataset):\n",
        "  f = open(os.path.join('/content/drive/MyDrive/CS512_Assignment/CatE/datasets', dataset, f'segmentation_{dataset}_phrase_processed.txt'))\n",
        "  docs = f.readlines()\n",
        "  out_file = f'/content/drive/MyDrive/CS512_Assignment/CatE/datasets/{dataset}/segmentation_{dataset}_phrase_lowercase_processed.txt'\n",
        "  f_out = open(out_file, 'w')\n",
        "  for doc in tqdm(docs):\n",
        "    f_out.write(' '.join([w.lower() for w in word_tokenize(doc.strip())]) + '\\n')\n",
        "  return\n",
        "\n",
        "\n",
        "dataset = 'news'\n",
        "#phrase_process(dataset)\n",
        "#preprocess(dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gGLdQkrPpvLo",
        "outputId": "8ab36dfc-6fd8-4fd5-fe6c-cfd0ba3d6aa5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 127600/127600 [00:57<00:00, 2204.67it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# edit run.sh for processing the movies dataset\n",
        "%cd /content/drive/MyDrive/CS512_Assignment/CatE\n",
        "!bash ./run.sh"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pzWkcmr_N_3H",
        "outputId": "da05761e-8b2d-4dbd-e07d-b08420fbea0a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/CS512_Assignment/CatE\n",
            "make: 'cate' is up to date.\n",
            "Starting training using file ./datasets/movies/segmentation_movies_phrase_lowercase_processed.txt\n",
            "Training with specificity; Specificity values output to file ./datasets/movies/emb_movies_category_spec.txt\n",
            "Reading topics from file ./datasets/movies/movies_category.txt\n",
            "Vocab size: 48672\n",
            "Words in train file: 12860776\n",
            "Loading embedding from file word2vec_100.txt\n",
            "In vocab: 36008\n",
            "Read 2 topics\n",
            "bad\t\n",
            "good\t\n",
            "Pre-training for 2 epochs, in total 2 + 10 = 12 epochs\n",
            "Alpha: 0.018752  Progress: 25.01%  Words/thread/sec: 34.06k  \n",
            "Category (bad): \tbad terrible \n",
            "Category (good): \tgood great \n",
            "Alpha: 0.016663  Progress: 33.35%  Words/thread/sec: 34.15k  \n",
            "Category (bad): \tbad unbelievably_bad terrible \n",
            "Category (good): \tgood solid all-together \n",
            "Alpha: 0.014581  Progress: 41.69%  Words/thread/sec: 34.04k  \n",
            "Category (bad): \tbad unbelievably_bad laughably_bad abysmal \n",
            "Category (good): \tgood all-together solid strong \n",
            "Alpha: 0.012495  Progress: 50.03%  Words/thread/sec: 33.86k  \n",
            "Category (bad): \tbad laughably_bad unbelievably_bad abysmal lousy \n",
            "Category (good): \tgood all-together solid strong terrific \n",
            "Alpha: 0.010410  Progress: 58.37%  Words/thread/sec: 33.96k  \n",
            "Category (bad): \tbad laughably_bad unbelievably_bad abysmal lousy atrocious \n",
            "Category (good): \tgood all-together solid strong terrific nice \n",
            "Alpha: 0.008327  Progress: 66.72%  Words/thread/sec: 34.04k  \n",
            "Category (bad): \tbad unbelievably_bad laughably_bad lousy abysmal atrocious horrid \n",
            "Category (good): \tgood all-together solid strong terrific nice fine \n",
            "Alpha: 0.006249  Progress: 75.02%  Words/thread/sec: 34.09k  \n",
            "Category (bad): \tbad unbelievably_bad lousy laughably_bad atrocious laughable abysmal horrid \n",
            "Category (good): \tgood solid all-together strong fine terrific nice excellent \n",
            "Alpha: 0.004160  Progress: 83.39%  Words/thread/sec: 34.08k  \n",
            "Category (bad): \tbad laughable awful lousy terrible atrocious unbelievably_bad pathetic horrible \n",
            "Category (good): \tgood solid strong all-together fine nice terrific excellent fine_job \n",
            "Alpha: 0.002076  Progress: 91.73%  Words/thread/sec: 34.03k  \n",
            "Category (bad): \tbad awful terrible horrible laughable lousy pathetic atrocious unbelievably_bad ridiculous \n",
            "Category (good): \tgood solid nice fine strong all-together excellent terrific fine_job outstanding \n",
            "Alpha: 0.000002  Progress: 100.07%  Words/thread/sec: 33.96k  \n",
            "Category (bad): \tbad awful terrible horrible laughable lousy ridiculous pathetic atrocious lame unbelievably_bad \n",
            "Category (good): \tgood nice solid fine strong great excellent also all-together terrific well \n",
            "Topic mining results written to file ./datasets/movies/res_movies_category.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# edit run.sh for processing the news dataset\n",
        "%cd /content/drive/MyDrive/CS512_Assignment/CatE\n",
        "!bash ./run.sh"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z9f1pLP29qpk",
        "outputId": "726cad66-fb31-49e0-b1c6-a3252fb80397"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/CS512_Assignment/CatE\n",
            "make: 'cate' is up to date.\n",
            "Starting training using file ./datasets/news/segmentation_news_phrase_lowercase_processed.txt\n",
            "Training with specificity; Specificity values output to file ./datasets/news/emb_news_category_spec.txt\n",
            "Reading topics from file ./datasets/news/news_category.txt\n",
            "Vocab size: 37269\n",
            "Words in train file: 5455819\n",
            "Loading embedding from file word2vec_100.txt\n",
            "In vocab: 25147\n",
            "Read 4 topics\n",
            "politics\t\n",
            "sports\t\n",
            "business\t\n",
            "technology\t\n",
            "Pre-training for 3 epochs, in total 3 + 10 = 13 epochs\n",
            "Alpha: 0.017467  Progress: 30.15%  Words/thread/sec: 31.97k  \n",
            "Category (politics): \tpolitics political \n",
            "Category (sports): \tsports network \n",
            "Category (business): \tbusiness company \n",
            "Category (technology): \ttechnology technologies \n",
            "Alpha: 0.015543  Progress: 37.84%  Words/thread/sec: 32.01k  \n",
            "Category (politics): \tpolitics political liberalism \n",
            "Category (sports): \tsports network team_report_-_august \n",
            "Category (business): \tbusiness company personal-computer \n",
            "Category (technology): \ttechnology technologies commercializing \n",
            "Alpha: 0.013620  Progress: 45.54%  Words/thread/sec: 32.10k  \n",
            "Category (politics): \tpolitics political liberalism ideological \n",
            "Category (sports): \tsports network team_report_-_august sportsnetwork_game_preview \n",
            "Category (business): \tbusiness personal-computer company wealth-management \n",
            "Category (technology): \ttechnology commercializing technologies 65-nanometer \n",
            "Alpha: 0.011696  Progress: 53.23%  Words/thread/sec: 32.13k  \n",
            "Category (politics): \tpolitics political liberalism ideological polarizing \n",
            "Category (sports): \tsports network team_report_-_august sportsnetwork_game_preview andreychuk \n",
            "Category (business): \tbusiness wealth-management personal-computer company 1.17 \n",
            "Category (technology): \ttechnology commercializing technologies 65-nanometer prototypes \n",
            "Alpha: 0.009773  Progress: 60.92%  Words/thread/sec: 32.10k  \n",
            "Category (politics): \tpolitics political liberalism polarizing ideological divisive \n",
            "Category (sports): \tsports network team_report_-_august sportsnetwork_game_preview andreychuk tx \n",
            "Category (business): \tbusiness wealth-management personal-computer 1.17 company business_jet \n",
            "Category (technology): \ttechnology commercializing 65-nanometer technologies prototypes proof-of-concept \n",
            "Alpha: 0.007849  Progress: 68.62%  Words/thread/sec: 32.13k  \n",
            "Category (politics): \tpolitics political liberalism polarizing ideological divisive agendas \n",
            "Category (sports): \tsports network team_report_-_august sportsnetwork_game_preview andreychuk tx team_report_-_december \n",
            "Category (business): \tbusiness wealth-management personal-computer 1.17 business_jet company wealth_management \n",
            "Category (technology): \ttechnology commercializing 65-nanometer technologies prototypes proof-of-concept internet_protocol_version_6 \n",
            "Alpha: 0.005926  Progress: 76.31%  Words/thread/sec: 32.17k  \n",
            "Category (politics): \tpolitics political liberalism agendas polarizing ideological misunderstood divisive \n",
            "Category (sports): \tsports network team_report_-_august sportsnetwork_game_preview andreychuk tx team_report_-_december ticker \n",
            "Category (business): \tbusiness wealth-management personal-computer 1.17 business_jet wealth_management customer unprofitable \n",
            "Category (technology): \ttechnology commercializing 65-nanometer technologies internet_protocol_version_6 proof-of-concept prototypes low-power \n",
            "Alpha: 0.004002  Progress: 84.00%  Words/thread/sec: 32.16k  \n",
            "Category (politics): \tpolitics political liberalism agendas polarizing ideological misunderstood divisive secular \n",
            "Category (sports): \tsports network team_report_-_august sportsnetwork_game_preview andreychuk tx team_report_-_december ticker ohio_university \n",
            "Category (business): \tbusiness wealth-management personal-computer 1.17 business_jet wealth_management customer unprofitable cigna \n",
            "Category (technology): \ttechnology commercializing 65-nanometer technologies internet_protocol_version_6 proof-of-concept prototypes low-power lcos \n",
            "Alpha: 0.002079  Progress: 91.70%  Words/thread/sec: 32.15k  \n",
            "Category (politics): \tpolitics political liberalism agendas polarizing ideological misunderstood secular adolf_hitler divisive \n",
            "Category (sports): \tsports network team_report_-_august sportsnetwork_game_preview andreychuk tx ohio_university team_report_-_december ticker fl \n",
            "Category (business): \tbusiness wealth-management personal-computer 1.17 business_jet wealth_management customer company unprofitable cigna \n",
            "Category (technology): \ttechnology commercializing 65-nanometer technologies internet_protocol_version_6 prototypes proof-of-concept low-power graphics_processing_unit lcos \n",
            "Alpha: 0.001576  Progress: 93.73%  Words/thread/sec: 32.16k  ^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Document Classification with CatE embeddings using WeSTClass"
      ],
      "metadata": {
        "id": "q6M2c6DhWWHi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#%cd /content/drive/MyDrive/CS512_Assignment\n",
        "\n",
        "#!git clone --quiet https://github.com/rfayat/spherecluster\n",
        "#!git clone --quiet https://github.com/yumeng5/WeSTClass\n",
        "\n",
        "#%cd /content/drive/MyDrive/CS512_Assignment/WeSTClass\n",
        "#!pip3 install -r requirements.txt"
      ],
      "metadata": {
        "id": "FA6F7Z1rWeUE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e3b35a6c-2b2b-4046-a767-8d6d3f08f0ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/CS512_Assignment/WeSTClass\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 1)) (1.23.5)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 2)) (3.8.1)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 3)) (4.3.2)\n",
            "Collecting sklearn (from -r requirements.txt (line 4))\n",
            "  Downloading sklearn-0.0.post10.tar.gz (3.6 kB)\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
            "\u001b[1;36mhint\u001b[0m: See above for details.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/CS512_Assignment/spherecluster/\n",
        "!python setup.py install"
      ],
      "metadata": {
        "id": "YCm9XLJ62Wdf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/CS512_Assignment/WeSTClass\n",
        "!bash ./test.sh"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6r_PybKeul-D",
        "outputId": "7ff98dfb-a482-4233-b3c7-46264d8fe039"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/CS512_Assignment/WeSTClass\n",
            "2023-10-27 03:14:37.745950: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-10-27 03:14:37.746015: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-10-27 03:14:37.746053: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-10-27 03:14:39.072693: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Namespace(dataset='agnews', model='cnn', sup_source='keywords', with_evaluation='True', batch_size=256, maxiter=5000.0, pretrain_epochs=None, update_interval=None, alpha=0.2, beta=500, gamma=50, delta=0.1, trained_weights=None)\n",
            "\n",
            "### Dataset statistics: ###\n",
            "Document max length: 225 (words)\n",
            "Document average length: 44.7625 (words)\n",
            "Document length std: 13.692216538968408 (words)\n",
            "Defined maximum document length: 100 (words)\n",
            "Fraction of truncated documents: 0.006625\n",
            "Number of classes: 4\n",
            "Number of documents in each class:\n",
            "Class 0: 30000\n",
            "Class 1: 30000\n",
            "Class 2: 30000\n",
            "Class 3: 30000\n",
            "Vocabulary Size: 67765\n",
            "\n",
            "### Supervision type: Class-related Keywords ###\n",
            "Keywords for each class: \n",
            "Supervision content of class 0:\n",
            "['government', 'military', 'war']\n",
            "Supervision content of class 1:\n",
            "['basketball', 'football', 'athletes']\n",
            "Supervision content of class 2:\n",
            "['stocks', 'markets', 'industries']\n",
            "Supervision content of class 3:\n",
            "['computer', 'telescope', 'software']\n",
            "\n",
            "### Input preparation ###\n",
            "Loading existing Word2Vec model ./agnews/embedding...\n",
            "/usr/local/lib/python3.10/dist-packages/keras/src/initializers/initializers.py:120: UserWarning: The initializer RandomUniform is unseeded and being called multiple times, which will return identical values each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initializer instance more than once.\n",
            "  warnings.warn(\n",
            "\n",
            "### Phase 1: vMF distribution fitting & pseudo document generation ###\n",
            "Retrieving top-t nearest words...\n",
            "Final expansion size t = 108\n",
            "Top-t nearest words for each class:\n",
            "Class 0:\n",
            "['military', 'government', 'war', 'insurgency', 'terrorism', 'syria', 'regime', 'army', 'genocide', 'taliban', 'rebels', 'conflict', 'afghanistan', 'invasion', 'country', 'lebanon', 'region', 'troops', 'ceasefire', 'pentagon', 'diplomatic', 'coalition', 'politicians', 'ambassador', 'arms', 'crisis', 'terrorist', 'terrorists', 'terror', 'sudan', 'movement', 'rwanda', 'violence', 'allies', 'militia', 'nato', 'israel', 'authorities', 'citizens', 'detention', 'political', 'occupation', 'fighters', 'declaration', 'border', 'tension', 'muslim', 'muslims', 'allawi', 'fighting', 'ethnic', 'intelligence', 'rebel', 'darfur', 'administration', 'un', 'territory', 'saddam', 'civil', 'serb', 'immigration', 'guerrillas', 'extradition', 'forces', 'parliament', 'attacks', 'independence', 'civilians', 'extremists', 'hussein', 'iraq', 'demands', 'iraqi', 'burma', 'resistance', 'colombia', 'congress', 'leadership', 'democracy', 'soldiers', 'insurgents', 'elections', 'corruption', 'tensions', 'junta', 'peacekeeping', 'foreign', 'myanmar', 'beirut', 'body', 'bosnia', 'province', 'libya', 'actions', 'hutu', 'disputed', 'islamic', 'cuba', 'congo', 'chechnya', 'ministry', 'fallujah', 'torture', 'commander', 'militants', 'sovereignty', 'tribunal', 'politics']\n",
            "Class 1:\n",
            "['basketball', 'football', 'coaches', 'athletes', 'hall', 'hockey', 'ncaa', 'college', 'playing', 'nba', 'fame', 'soccer', 'fans', 'players', 'artest', 'talented', 'ron', 'coaching', 'english', 'indiana', 'golden', 'brawl', 'pride', 'keith', 'athletic', 'boys', 'greatest', 'young', 'crocker', 'boxing', 'professional', 'willingham', 'stars', 'elite', 'miller', 'honor', 'sportsmanship', 'star', 'men', 'sport', 'pacers', 'mia', 'team', 'winningest', 'experience', 'teams', 'suspensions', 'bronze', 'walter', 'utah', 'youth', 'famous', 'maurice', 'nfl', 'bc', 'probation', 'greene', 'chants', 'lou', 'sprinters', 'harvard', 'coach', 'fan', 'hoops', 'lightweight', 'jackson', 'wrestling', 'squad', 'history', 'heisman', 'player', 'tournaments', 'winners', 'hansen', 'quarterbacks', 'murphy', 'women', 'junior', 'played', 'huskies', 'jeremy', 'superstar', 'undefeated', 'cal', 'ugly', 'pool', 'acc', 'clubs', 'athlete', 'nhl', 'dame', 'mora', 'volleyball', 'play', 'auburn', 'justin', 'stadiums', 'inductees', 'bowl', 'vols', 'oxford', 'games', 'bruin', 'michigan', 'cfl', 'stephen', 'dream', 'clarett']\n",
            "Class 2:\n",
            "['markets', 'stocks', 'exporters', 'sectors', 'trading', 'shares', 'gains', 'metals', 'sector', 'industries', 'investors', 'profits', 'volume', 'prices', 'traders', 'goods', 'manufacturers', 'downbeat', 'weighed', 'index', 'stock', 'sharply', 'firms', 'manufacturing', 'currency', 'techs', 'companies', 'investments', 'commodities', 'producers', 'ltd', 'sachs', 'industrial', 'banks', 'dow', 'tsx', 'earnings', 'losses', 'limited', 'firmer', 'semiconductor', 'cola', 'investment', 'market', 'futures', 'weakness', 'equities', 'inventory', 'equity', 'makers', 'colgate', 'economies', 'domestic', 'acquisitions', 'brokerage', 'slightly', 'chains', 'output', 'bond', 'goldman', 'unilever', 'cautious', 'outlook', 'higher', 'insurers', 'brands', 'hutchison', 'chips', 'advantest', 'sentiment', 'greenback', 'downgrade', 'bellwether', 'revenues', 'indexes', 'lending', 'gloomy', 'declines', 'buyers', 'bhp', 'currencies', 'textiles', 'treasuries', 'demand', 'expenses', 'integris', 'banking', 'lower', 'purchases', 'diesel', 'imports', 'posco', 'forecasts', 'exchange', 'drinks', 'artisan', 'costs', 'assets', 'profit', 'spr', 'metcash', 'steel', 'inventories', 'retreated', 'buying', 'funds', 'indicators', 'products']\n",
            "Class 3:\n",
            "['computer', 'software', 'design', 'computers', 'pcs', 'servers', 'tools', 'technology', 'firewall', 'user', 'built', 'automatically', 'system', 'desktops', 'tool', 'database', 'telescope', 'image', 'images', 'multiple', 'machines', 'cluster', 'virus', 'hackers', 'robot', 'architecture', 'fingerprint', 'machine', 'malicious', 'spyware', 'hacker', 'interface', 'language', 'os', 'application', 'computing', 'capabilities', 'content', 'printer', 'ie', 'personal', 'functions', 'pc', 'glitches', 'technical', 'processing', 'designs', 'screen', 'programming', 'mouse', 'laptop', 'code', 'virtual', 'storage', 'java', 'worms', 'feature', 'email', 'distribution', 'flaw', 'disk', 'scanning', 'mac', 'faulty', 'unix', 'malware', 'linux', 'enterprise', 'instant', 'antivirus', 'plug', 'sleek', 'model', 'digital', 'audio', 'vulnerability', 'reader', 'features', 'graphics', 'windows', 'desktop', 'product', 'device', 'platform', 'ipod', 'technique', 'layer', 'worm', 'integrated', 'keyboard', 'unique', 'scanner', 'module', 'download', 'integrates', 'fix', 'netscape', 'planet', 'compromised', 'copy', 'ipods', 'dell', 'mainframe', 'innovation', 'users', 'passwords', 'blogging', 'ibm']\n",
            "Finished vMF distribution fitting.\n",
            "Pseudo documents generation...\n",
            "Finished Pseudo documents generation.\n",
            "\n",
            "### Phase 2: pre-training with pseudo documents ###\n",
            "/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/legacy/gradient_descent.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n",
            "\n",
            "Neural model summary: \n",
            "Model: \"classifier\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input (InputLayer)          [(None, 100)]                0         []                            \n",
            "                                                                                                  \n",
            " embedding (Embedding)       (None, 100, 100)             6776500   ['input[0][0]']               \n",
            "                                                                                                  \n",
            " conv1d (Conv1D)             (None, 99, 20)               4020      ['embedding[0][0]']           \n",
            "                                                                                                  \n",
            " conv1d_1 (Conv1D)           (None, 98, 20)               6020      ['embedding[0][0]']           \n",
            "                                                                                                  \n",
            " conv1d_2 (Conv1D)           (None, 97, 20)               8020      ['embedding[0][0]']           \n",
            "                                                                                                  \n",
            " conv1d_3 (Conv1D)           (None, 96, 20)               10020     ['embedding[0][0]']           \n",
            "                                                                                                  \n",
            " global_max_pooling1d (Glob  (None, 20)                   0         ['conv1d[0][0]']              \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_1 (Gl  (None, 20)                   0         ['conv1d_1[0][0]']            \n",
            " obalMaxPooling1D)                                                                                \n",
            "                                                                                                  \n",
            " global_max_pooling1d_2 (Gl  (None, 20)                   0         ['conv1d_2[0][0]']            \n",
            " obalMaxPooling1D)                                                                                \n",
            "                                                                                                  \n",
            " global_max_pooling1d_3 (Gl  (None, 20)                   0         ['conv1d_3[0][0]']            \n",
            " obalMaxPooling1D)                                                                                \n",
            "                                                                                                  \n",
            " concatenate (Concatenate)   (None, 80)                   0         ['global_max_pooling1d[0][0]',\n",
            "                                                                     'global_max_pooling1d_1[0][0]\n",
            "                                                                    ',                            \n",
            "                                                                     'global_max_pooling1d_2[0][0]\n",
            "                                                                    ',                            \n",
            "                                                                     'global_max_pooling1d_3[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " dense (Dense)               (None, 20)                   1620      ['concatenate[0][0]']         \n",
            "                                                                                                  \n",
            " dense_1 (Dense)             (None, 4)                    84        ['dense[0][0]']               \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 6806284 (25.96 MB)\n",
            "Trainable params: 29784 (116.34 KB)\n",
            "Non-trainable params: 6776500 (25.85 MB)\n",
            "__________________________________________________________________________________________________\n",
            "\n",
            "Pretraining...\n",
            "Epoch 1/20\n",
            "8/8 [==============================] - 2s 143ms/step - loss: 0.7141\n",
            "Epoch 2/20\n",
            "8/8 [==============================] - 1s 140ms/step - loss: 0.2604\n",
            "Epoch 3/20\n",
            "8/8 [==============================] - 1s 141ms/step - loss: 0.0642\n",
            "Epoch 4/20\n",
            "8/8 [==============================] - 1s 141ms/step - loss: 0.0340\n",
            "Epoch 5/20\n",
            "8/8 [==============================] - 1s 141ms/step - loss: 0.0225\n",
            "Epoch 6/20\n",
            "8/8 [==============================] - 1s 140ms/step - loss: 0.0080\n",
            "Epoch 7/20\n",
            "8/8 [==============================] - 1s 144ms/step - loss: 0.0067\n",
            "Epoch 8/20\n",
            "8/8 [==============================] - 2s 250ms/step - loss: 0.0048\n",
            "Epoch 9/20\n",
            "8/8 [==============================] - 2s 246ms/step - loss: 0.0036\n",
            "Epoch 10/20\n",
            "8/8 [==============================] - 2s 243ms/step - loss: 0.0031\n",
            "Epoch 11/20\n",
            "8/8 [==============================] - 1s 176ms/step - loss: 0.0028\n",
            "Epoch 12/20\n",
            "8/8 [==============================] - 1s 141ms/step - loss: 0.0025\n",
            "Epoch 13/20\n",
            "8/8 [==============================] - 1s 142ms/step - loss: 0.0024\n",
            "Epoch 14/20\n",
            "8/8 [==============================] - 1s 142ms/step - loss: 0.0023\n",
            "Epoch 15/20\n",
            "8/8 [==============================] - 1s 139ms/step - loss: 0.0022\n",
            "Epoch 16/20\n",
            "8/8 [==============================] - 1s 141ms/step - loss: 0.0021\n",
            "Epoch 17/20\n",
            "8/8 [==============================] - 1s 138ms/step - loss: 0.0020\n",
            "Epoch 18/20\n",
            "8/8 [==============================] - 1s 139ms/step - loss: 0.0019\n",
            "Epoch 19/20\n",
            "8/8 [==============================] - 1s 141ms/step - loss: 0.0019\n",
            "Epoch 20/20\n",
            "8/8 [==============================] - 2s 223ms/step - loss: 0.0018\n",
            "Pretraining time: 26.95s\n",
            "Pretrained model saved to ./results/agnews/cnn/phase2/pretrained.h5\n",
            "F1 score after pre-training: f1_macro = 0.74188, f1_micro = 0.74527\n",
            "\n",
            "### Phase 3: self-training ###\n",
            "/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/legacy/gradient_descent.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n",
            "Update interval: 50\n",
            "3750/3750 [==============================] - 36s 9ms/step\n",
            "\n",
            "Iter 0: f1_macro = 0.74188, f1_micro = 0.74527\n",
            "Fraction of documents with label changes: 0.0 %\n",
            "\n",
            "Iter 50: f1_macro = 0.76475, f1_micro = 0.76832\n",
            "Fraction of documents with label changes: 3.81 %\n",
            "\n",
            "Iter 100: f1_macro = 0.77773, f1_micro = 0.78118\n",
            "Fraction of documents with label changes: 2.339 %\n",
            "\n",
            "Iter 150: f1_macro = 0.78799, f1_micro = 0.79115\n",
            "Fraction of documents with label changes: 2.048 %\n",
            "\n",
            "Iter 200: f1_macro = 0.79483, f1_micro = 0.79781\n",
            "Fraction of documents with label changes: 1.523 %\n",
            "\n",
            "Iter 250: f1_macro = 0.80077, f1_micro = 0.80358\n",
            "Fraction of documents with label changes: 1.63 %\n",
            "\n",
            "Iter 300: f1_macro = 0.80285, f1_micro = 0.80567\n",
            "Fraction of documents with label changes: 0.795 %\n",
            "\n",
            "Iter 350: f1_macro = 0.80446, f1_micro = 0.80722\n",
            "Fraction of documents with label changes: 0.592 %\n",
            "\n",
            "Iter 400: f1_macro = 0.80542, f1_micro = 0.8082\n",
            "Fraction of documents with label changes: 0.373 %\n",
            "\n",
            "Iter 450: f1_macro = 0.80655, f1_micro = 0.80927\n",
            "Fraction of documents with label changes: 0.452 %\n",
            "\n",
            "Iter 500: f1_macro = 0.80789, f1_micro = 0.81059\n",
            "Fraction of documents with label changes: 0.489 %\n",
            "\n",
            "Iter 550: f1_macro = 0.80908, f1_micro = 0.81175\n",
            "Fraction of documents with label changes: 0.3 %\n",
            "\n",
            "Iter 600: f1_macro = 0.81006, f1_micro = 0.81266\n",
            "Fraction of documents with label changes: 0.217 %\n",
            "\n",
            "Iter 650: f1_macro = 0.8112, f1_micro = 0.81372\n",
            "Fraction of documents with label changes: 0.438 %\n",
            "\n",
            "Iter 700: f1_macro = 0.81192, f1_micro = 0.81442\n",
            "Fraction of documents with label changes: 0.4 %\n",
            "\n",
            "Iter 750: f1_macro = 0.81255, f1_micro = 0.81503\n",
            "Fraction of documents with label changes: 0.136 %\n",
            "\n",
            "Iter 800: f1_macro = 0.81302, f1_micro = 0.81549\n",
            "Fraction of documents with label changes: 0.228 %\n",
            "\n",
            "Iter 850: f1_macro = 0.81305, f1_micro = 0.81554\n",
            "Fraction of documents with label changes: 0.257 %\n",
            "\n",
            "Iter 900: f1_macro = 0.81424, f1_micro = 0.81663\n",
            "Fraction of documents with label changes: 0.299 %\n",
            "\n",
            "Iter 950: f1_macro = 0.81518, f1_micro = 0.81751\n",
            "Fraction of documents with label changes: 0.21 %\n",
            "\n",
            "Iter 1000: f1_macro = 0.81596, f1_micro = 0.81825\n",
            "Fraction of documents with label changes: 0.172 %\n",
            "\n",
            "Iter 1050: f1_macro = 0.8165, f1_micro = 0.81877\n",
            "Fraction of documents with label changes: 0.195 %\n",
            "\n",
            "Iter 1100: f1_macro = 0.81683, f1_micro = 0.81908\n",
            "Fraction of documents with label changes: 0.229 %\n",
            "\n",
            "Iter 1150: f1_macro = 0.81798, f1_micro = 0.82019\n",
            "Fraction of documents with label changes: 0.238 %\n",
            "\n",
            "Iter 1200: f1_macro = 0.81901, f1_micro = 0.82114\n",
            "Fraction of documents with label changes: 0.279 %\n",
            "\n",
            "Iter 1250: f1_macro = 0.81944, f1_micro = 0.82155\n",
            "Fraction of documents with label changes: 0.162 %\n",
            "\n",
            "Iter 1300: f1_macro = 0.81953, f1_micro = 0.82166\n",
            "Fraction of documents with label changes: 0.131 %\n",
            "\n",
            "Iter 1350: f1_macro = 0.8201, f1_micro = 0.82222\n",
            "Fraction of documents with label changes: 0.182 %\n",
            "\n",
            "Iter 1400: f1_macro = 0.82086, f1_micro = 0.82291\n",
            "Fraction of documents with label changes: 0.238 %\n",
            "\n",
            "Iter 1450: f1_macro = 0.82162, f1_micro = 0.82362\n",
            "Fraction of documents with label changes: 0.202 %\n",
            "\n",
            "Iter 1500: f1_macro = 0.82193, f1_micro = 0.82392\n",
            "Fraction of documents with label changes: 0.193 %\n",
            "\n",
            "Iter 1550: f1_macro = 0.82194, f1_micro = 0.82396\n",
            "Fraction of documents with label changes: 0.077 %\n",
            "\n",
            "Fraction: 0.077 % < tol: 0.1 %\n",
            "Reached tolerance threshold. Stopping training.\n",
            "Final model saved to: ./results/agnews/cnn/phase3/final.h5\n",
            "Self-training time: 1606.93s\n",
            "\n",
            "### Generating outputs ###\n",
            "Classification results are written in ./agnews/out.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/CS512_Assignment/WeSTClass/\n",
        "!python main.py --dataset 'movies' --sup_source 'keywords' --model 'cnn' --with_evaluation 'False' --pretrain_epochs 5"
      ],
      "metadata": {
        "id": "JY_PwnW7uIra",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd2495b3-8fcd-484d-a6fa-e70f82a8632e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/CS512_Assignment/WeSTClass\n",
            "2023-10-27 06:05:48.637004: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-10-27 06:05:48.637075: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-10-27 06:05:48.637112: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-10-27 06:05:48.646944: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-10-27 06:05:50.394535: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Namespace(dataset='movies', model='cnn', sup_source='keywords', with_evaluation='False', batch_size=256, maxiter=5000.0, pretrain_epochs=5, update_interval=None, alpha=0.2, beta=500, gamma=50, delta=0.1, trained_weights=None)\n",
            "\n",
            "### Dataset statistics: ###\n",
            "Document max length: 2660 (words)\n",
            "Document average length: 261.02756 (words)\n",
            "Document length std: 192.72532838328854 (words)\n",
            "Defined maximum document length: 500 (words)\n",
            "Fraction of truncated documents: 0.10338\n",
            "Vocabulary Size: 157972\n",
            "\n",
            "### Supervision type: Class-related Keywords ###\n",
            "Keywords for each class: \n",
            "Supervision content of class 0:\n",
            "['awful', 'terrible', 'horrible', 'laughable', 'ridiculous', 'pathetic', 'lame', 'lousy', 'atrocious', 'unbelievably_bad']\n",
            "Supervision content of class 1:\n",
            "['nice', 'solid', 'fine', 'strong', 'also', 'great', 'excellent', 'terrific', 'all-together', 'well']\n",
            "\n",
            "### Input preparation ###\n",
            "Loading existing Word2Vec model ./movies/embedding...\n",
            "/usr/local/lib/python3.10/dist-packages/keras/src/initializers/initializers.py:120: UserWarning: The initializer RandomUniform is unseeded and being called multiple times, which will return identical values each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initializer instance more than once.\n",
            "  warnings.warn(\n",
            "\n",
            "### Phase 1: vMF distribution fitting & pseudo document generation ###\n",
            "Retrieving top-t nearest words...\n",
            "Final expansion size t = 80\n",
            "Top-t nearest words for each class:\n",
            "Class 0:\n",
            "['terrible', 'horrible', 'awful', 'pathetic', 'laughable', 'dreadful', 'horrendous', 'atrocious', 'ridiculous', 'lousy', 'horrid', 'lame', 'abysmal', 'woeful', 'stupid', 'substandard', 'bad', 'amateurish', 'asinine', 'dialouge', 'dire', 'appalling', 'pitiful', 'ludicrous', 'godawful', 'deplorable', 'disgraceful', 'crappy', 'abominable', 'shoddy', 'irredeemably', 'horrendously', 'laughably', 'idiotic', 'scriptwriting', 'unbelievable', 'dismal', 'sucky', 'wretched', 'horrifically', 'mediocre', 'wretchedly', 'embarrassingly', 'incoherent', 'crummy', 'patchy', 'poor', 'slapdash', 'unprofessional', 'silly', 'iffy', 'cheesy', 'offensively', 'redundant', 'imbecilic', 'unendurable', 'absurd', 'putrid', 'unsexy', 'nonsensical', 'plotwise', 'pointless', 'hideous', 'cheezy', 'plotless', 'lacklustre', 'listless', 'unrealistic', 'disposable', 'rubbish', 'dreadfully', 'predicable', 'unwatchable', 'uncreative', 'uninspired', 'ridicules', 'pathetically', 'useless', 'sequencing', 'unimaginative']\n",
            "Class 1:\n",
            "['terrific', 'fine', 'great', 'excellent', 'solid', 'superb', 'good', 'fantastic', 'wonderful', 'marvellous', 'nice', 'awsome', 'outstanding', 'brilliant', 'splendid', 'marvelous', 'superlative', 'exceptional', 'strong', 'phenomenal', 'fabulous', 'feelgood', 'serviceable', 'powerhouse', 'decent', 'scarily', 'excelent', 'creditable', 'matondkar', 'sturdy', 'canny', 'workable', 'faultless', 'clannad', 'debuting', 'stellar', 'wittiness', 'commendable', 'unexceptional', 'breezy', 'terrificly', 'impacting', 'aslan', 'picturization', 'notch', 'atypically', 'complements', 'renaldo', 'uncharismatic', 'mesmerising', 'gustafsson', 'glimmers', 'starck', 'locusts', 'kungfu', 'stupendous', 'anchored', 'drury', 'mceveety', 'unspectacular', 'organically', 'slipshod', 'underplayed', 'spectaculars', 'masterclass', 'masterstroke', 'subpar', 'puyn', 'resplendent', 'entrancing', 'complementary', 'juicy', 'magnificent', 'underutilized', 'mckeon', 'gutsy', 'ideally', 'impeccable', 'scattershot', 'proficient']\n",
            "Finished vMF distribution fitting.\n",
            "Pseudo documents generation...\n",
            "Finished Pseudo documents generation.\n",
            "\n",
            "### Phase 2: pre-training with pseudo documents ###\n",
            "/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/legacy/gradient_descent.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n",
            "\n",
            "Neural model summary: \n",
            "Model: \"classifier\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input (InputLayer)          [(None, 500)]                0         []                            \n",
            "                                                                                                  \n",
            " embedding (Embedding)       (None, 500, 100)             1579720   ['input[0][0]']               \n",
            "                                                          0                                       \n",
            "                                                                                                  \n",
            " conv1d (Conv1D)             (None, 499, 20)              4020      ['embedding[0][0]']           \n",
            "                                                                                                  \n",
            " conv1d_1 (Conv1D)           (None, 498, 20)              6020      ['embedding[0][0]']           \n",
            "                                                                                                  \n",
            " conv1d_2 (Conv1D)           (None, 497, 20)              8020      ['embedding[0][0]']           \n",
            "                                                                                                  \n",
            " conv1d_3 (Conv1D)           (None, 496, 20)              10020     ['embedding[0][0]']           \n",
            "                                                                                                  \n",
            " global_max_pooling1d (Glob  (None, 20)                   0         ['conv1d[0][0]']              \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_1 (Gl  (None, 20)                   0         ['conv1d_1[0][0]']            \n",
            " obalMaxPooling1D)                                                                                \n",
            "                                                                                                  \n",
            " global_max_pooling1d_2 (Gl  (None, 20)                   0         ['conv1d_2[0][0]']            \n",
            " obalMaxPooling1D)                                                                                \n",
            "                                                                                                  \n",
            " global_max_pooling1d_3 (Gl  (None, 20)                   0         ['conv1d_3[0][0]']            \n",
            " obalMaxPooling1D)                                                                                \n",
            "                                                                                                  \n",
            " concatenate (Concatenate)   (None, 80)                   0         ['global_max_pooling1d[0][0]',\n",
            "                                                                     'global_max_pooling1d_1[0][0]\n",
            "                                                                    ',                            \n",
            "                                                                     'global_max_pooling1d_2[0][0]\n",
            "                                                                    ',                            \n",
            "                                                                     'global_max_pooling1d_3[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " dense (Dense)               (None, 20)                   1620      ['concatenate[0][0]']         \n",
            "                                                                                                  \n",
            " dense_1 (Dense)             (None, 2)                    42        ['dense[0][0]']               \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 15826942 (60.37 MB)\n",
            "Trainable params: 29742 (116.18 KB)\n",
            "Non-trainable params: 15797200 (60.26 MB)\n",
            "__________________________________________________________________________________________________\n",
            "\n",
            "Pretraining...\n",
            "Epoch 1/5\n",
            "4/4 [==============================] - 6s 1s/step - loss: 0.3641\n",
            "Epoch 2/5\n",
            "4/4 [==============================] - 4s 817ms/step - loss: 0.3173\n",
            "Epoch 3/5\n",
            "4/4 [==============================] - 3s 724ms/step - loss: 0.1942\n",
            "Epoch 4/5\n",
            "4/4 [==============================] - 3s 707ms/step - loss: 0.0513\n",
            "Epoch 5/5\n",
            "4/4 [==============================] - 3s 747ms/step - loss: 0.0208\n",
            "Pretraining time: 18.71s\n",
            "Pretrained model saved to ./results/movies/cnn/phase2/pretrained.h5\n",
            "\n",
            "### Phase 3: self-training ###\n",
            "Update interval: 50\n",
            "1563/1563 [==============================] - 59s 38ms/step\n",
            "\n",
            "Iter 0: Fraction of documents with label changes: 0.0 %\n",
            "\n",
            "Iter 50: Fraction of documents with label changes: 1.174 %\n",
            "\n",
            "Iter 100: Fraction of documents with label changes: 1.388 %\n",
            "\n",
            "Iter 150: Fraction of documents with label changes: 1.246 %\n",
            "\n",
            "Iter 200: Fraction of documents with label changes: 1.26 %\n",
            "\n",
            "Iter 250: Fraction of documents with label changes: 1.02 %\n",
            "\n",
            "Iter 300: Fraction of documents with label changes: 1.072 %\n",
            "\n",
            "Iter 350: Fraction of documents with label changes: 1.054 %\n",
            "\n",
            "Iter 400: Fraction of documents with label changes: 0.98 %\n",
            "\n",
            "Iter 450: Fraction of documents with label changes: 0.818 %\n",
            "\n",
            "Iter 500: Fraction of documents with label changes: 0.798 %\n",
            "\n",
            "Iter 550: Fraction of documents with label changes: 0.69 %\n",
            "\n",
            "Iter 600: Fraction of documents with label changes: 0.812 %\n",
            "\n",
            "Iter 650: Fraction of documents with label changes: 0.628 %\n",
            "\n",
            "Iter 700: Fraction of documents with label changes: 0.712 %\n",
            "\n",
            "Iter 750: Fraction of documents with label changes: 0.568 %\n",
            "\n",
            "Iter 800: Fraction of documents with label changes: 0.57 %\n",
            "\n",
            "Iter 850: Fraction of documents with label changes: 0.488 %\n",
            "\n",
            "Iter 900: Fraction of documents with label changes: 0.472 %\n",
            "\n",
            "Iter 950: Fraction of documents with label changes: 0.448 %\n",
            "\n",
            "Iter 1000: Fraction of documents with label changes: 0.404 %\n",
            "\n",
            "Iter 1050: Fraction of documents with label changes: 0.348 %\n",
            "\n",
            "Iter 1100: Fraction of documents with label changes: 0.34 %\n",
            "\n",
            "Iter 1150: Fraction of documents with label changes: 0.268 %\n",
            "\n",
            "Iter 1200: Fraction of documents with label changes: 0.31 %\n",
            "\n",
            "Iter 1250: Fraction of documents with label changes: 0.228 %\n",
            "\n",
            "Iter 1300: Fraction of documents with label changes: 0.252 %\n",
            "\n",
            "Iter 1350: Fraction of documents with label changes: 0.266 %\n",
            "\n",
            "Iter 1400: Fraction of documents with label changes: 0.26 %\n",
            "\n",
            "Iter 1450: Fraction of documents with label changes: 0.148 %\n",
            "\n",
            "Iter 1500: Fraction of documents with label changes: 0.18 %\n",
            "\n",
            "Iter 1550: Fraction of documents with label changes: 0.248 %\n",
            "\n",
            "Iter 1600: Fraction of documents with label changes: 0.128 %\n",
            "\n",
            "Iter 1650: Fraction of documents with label changes: 0.138 %\n",
            "\n",
            "Iter 1700: Fraction of documents with label changes: 0.088 %\n",
            "\n",
            "Fraction: 0.088 % < tol: 0.1 %\n",
            "Reached tolerance threshold. Stopping training.\n",
            "Final model saved to: ./results/movies/cnn/phase3/final.h5\n",
            "Self-training time: 4176.92s\n",
            "\n",
            "### Generating outputs ###\n",
            "Classification results are written in ./movies/out.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/CS512_Assignment/WeSTClass/\n",
        "!python main.py --dataset 'news' --sup_source 'keywords' --model 'cnn' --with_evaluation 'False' --maxiter 50000 --pretrain_epochs 5"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iW9PfFCjfW4U",
        "outputId": "e4d226a3-8915-47f5-c3a6-0705c6142bfc"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/CS512_Assignment/WeSTClass\n",
            "2023-10-27 16:41:16.528542: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-10-27 16:41:16.528616: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-10-27 16:41:16.528642: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-10-27 16:41:16.534629: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-10-27 16:41:17.516951: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Namespace(dataset='news', model='cnn', sup_source='keywords', with_evaluation='False', batch_size=256, maxiter=50000, pretrain_epochs=5, update_interval=None, alpha=0.2, beta=500, gamma=50, delta=0.1, trained_weights=None)\n",
            "\n",
            "### Dataset statistics: ###\n",
            "Document max length: 202 (words)\n",
            "Document average length: 41.275188087774296 (words)\n",
            "Document length std: 12.11052194316095 (words)\n",
            "Defined maximum document length: 100 (words)\n",
            "Fraction of truncated documents: 0.003252351097178683\n",
            "Vocabulary Size: 106496\n",
            "\n",
            "### Supervision type: Class-related Keywords ###\n",
            "Keywords for each class: \n",
            "Supervision content of class 0:\n",
            "['liberalism', 'agendas', 'political', 'undemocratic', 'fundamentalist', 'polarizing', 'ideological', 'misunderstood', 'adolf_hitler', 'extremism']\n",
            "Supervision content of class 1:\n",
            "['network', 'team_report_-_august', 'andreychuk', 'sportsnetwork_game_preview', 'ticker', 'tx', 'ny_jets', 'sportsticker', 'team_report_-_december', 'fl']\n",
            "Supervision content of class 2:\n",
            "['personal-computer', 'pc-making', 'wealth-management', 'systemcorp', 'customer', 'lenovo', 'majority_stake', 'wealth_management', '1.17', 'ubs_ag']\n",
            "Supervision content of class 3:\n",
            "['commercializing', 'touch_screen', 'internet_protocol_version_6', 'graphics_processing_unit', 'technologies', 'ieee', 'second-generation', 'low-power', '65-nanometer', 'proof-of-concept']\n",
            "\n",
            "### Input preparation ###\n",
            "Loading existing Word2Vec model ./news/embedding...\n",
            "/usr/local/lib/python3.10/dist-packages/keras/src/initializers/initializers.py:120: UserWarning: The initializer RandomUniform is unseeded and being called multiple times, which will return identical values each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initializer instance more than once.\n",
            "  warnings.warn(\n",
            "\n",
            "### Phase 1: vMF distribution fitting & pseudo document generation ###\n",
            "Retrieving top-t nearest words...\n",
            "Final expansion size t = 55\n",
            "Top-t nearest words for each class:\n",
            "Class 0:\n",
            "['lesbians', 'groomed', 'uncompromising', 'resorting', 'friendships', 'fundamentalist', 'anarchists', 'communism', 'constituency', 'sermons', 'preparedness', 'appointees', 'periodic', 'conspicuous', 'pacifism', 'parallels', 'intimidating', 'coherent', 'imbroglio', 'obsessed', 'affiliation', 'civilized', 'distorted', 'grassroots', 'mistrust', 'regimes', 'anxieties', 'discriminatory', 'ideological', 'misunderstood', 'beliefs', 'hindering', 'peacemaker', 'hazing', 'dilemmas', 'heirs', 'evangelical', 'recounted', 'prejudice', 'covenant', 'agendas', 'intervening', 'elevating', 'notoriety', 'geo', 'enlist', 'inefficient', 'minefield', 'fomenting', 'distrust', 'renounced', 'reconcile', 'irreversible', 'resentment', 'lectures']\n",
            "Class 1:\n",
            "['sportsticker', 'tx', 'pcl', 'stillwater', 'fl', 'berea', 'easton', 'ps', 'tn', 'lhp', 'nallen', 'wa', 'dominguez', 'maroth', 'niners', 'tempe', 'ir', 'janikowski', 'antoine', 'weei', 'englewood', 'alds', 'dh', 'brazelton', 'jarrod', 'ollie', 'ivey', 'taquan', 'mantei', 'wwzn', 'knoxville', 'clobber', 'chone', 'morten', 'cheryl', 'scobee', 'mi', 'ss', 'avondale', 'lexington', 'mccoy', 'ritchie', 'inf', 'buckley', 'szczerbiak', 'hambrick', 'pummel', 'activate', 'hempstead', 'fassero', 'ct', 'alameda', 'vikes', 'sweepstakes', 'daisuke']\n",
            "Class 2:\n",
            "['10g', 'systemcorp', 'ameren', 'reorganized', 'brightstor', 'simplifies', '430m', 'perfigo', 'aix', 'southtrust', 'citrix', 'pathscale', 'logistics', '11i', 'corel', 'gmbh', 'venetica', 'netegrity', 'clearwire', 'provisioning', 'airwave', 'linksys', 'f5', 'bmc', 'sbs', 'mediacom', 'lifecycle', 'kvs', 'fibre', 'net6', 'oem', 'netweaver', 'ilm', 'middleware', 'isvs', 'refine', 'connector', 'suites', 'totalstorage', 'vpn', 'tivoli', 'rebate', 'complexity', 'openmanage', 'wlan', 'quickbooks', 'canwest', 'ios', 'streamline', 'weblogic', 'architecture', 'unicenter', '4ghz', 'backbone', '225m']\n",
            "Class 3:\n",
            "['ieee', 'lto', 'multimedia', 'compression', 'interface', 'incorporates', 'preminet', 'cdma', 'adapter', 'suretype', 'smp', 'tagmastore', 'cherryos', 'editing', 'objectweb', 'radeon', 'av', '1855', '5gb', 'logitech', 'smartphone', 'communicator', 'workstation', 'hds', 'iseries', 'inexpensive', '6200', '802', 'copier', '60gb', '9300', 'interfaces', 'storage', 'agp', 'api', 'thinkpad', 'computerized', 'intuitive', 'bladecenter', 'domainkeys', 'mdm', 'soa', 'marketed', 'specs', 'poweredge', 'qwerty', 'hobbyists', 'subsystem', 'singingfish', 'ultrasparc', 'groklaw', 'scanner', 'toolset', 'x850', 'dwango']\n",
            "Finished vMF distribution fitting.\n",
            "Pseudo documents generation...\n",
            "Finished Pseudo documents generation.\n",
            "\n",
            "### Phase 2: pre-training with pseudo documents ###\n",
            "/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/legacy/gradient_descent.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n",
            "\n",
            "Neural model summary: \n",
            "Model: \"classifier\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input (InputLayer)          [(None, 100)]                0         []                            \n",
            "                                                                                                  \n",
            " embedding (Embedding)       (None, 100, 100)             1064960   ['input[0][0]']               \n",
            "                                                          0                                       \n",
            "                                                                                                  \n",
            " conv1d (Conv1D)             (None, 99, 20)               4020      ['embedding[0][0]']           \n",
            "                                                                                                  \n",
            " conv1d_1 (Conv1D)           (None, 98, 20)               6020      ['embedding[0][0]']           \n",
            "                                                                                                  \n",
            " conv1d_2 (Conv1D)           (None, 97, 20)               8020      ['embedding[0][0]']           \n",
            "                                                                                                  \n",
            " conv1d_3 (Conv1D)           (None, 96, 20)               10020     ['embedding[0][0]']           \n",
            "                                                                                                  \n",
            " global_max_pooling1d (Glob  (None, 20)                   0         ['conv1d[0][0]']              \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_1 (Gl  (None, 20)                   0         ['conv1d_1[0][0]']            \n",
            " obalMaxPooling1D)                                                                                \n",
            "                                                                                                  \n",
            " global_max_pooling1d_2 (Gl  (None, 20)                   0         ['conv1d_2[0][0]']            \n",
            " obalMaxPooling1D)                                                                                \n",
            "                                                                                                  \n",
            " global_max_pooling1d_3 (Gl  (None, 20)                   0         ['conv1d_3[0][0]']            \n",
            " obalMaxPooling1D)                                                                                \n",
            "                                                                                                  \n",
            " concatenate (Concatenate)   (None, 80)                   0         ['global_max_pooling1d[0][0]',\n",
            "                                                                     'global_max_pooling1d_1[0][0]\n",
            "                                                                    ',                            \n",
            "                                                                     'global_max_pooling1d_2[0][0]\n",
            "                                                                    ',                            \n",
            "                                                                     'global_max_pooling1d_3[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " dense (Dense)               (None, 20)                   1620      ['concatenate[0][0]']         \n",
            "                                                                                                  \n",
            " dense_1 (Dense)             (None, 4)                    84        ['dense[0][0]']               \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 10679384 (40.74 MB)\n",
            "Trainable params: 29784 (116.34 KB)\n",
            "Non-trainable params: 10649600 (40.62 MB)\n",
            "__________________________________________________________________________________________________\n",
            "\n",
            "Pretraining...\n",
            "Epoch 1/5\n",
            "8/8 [==============================] - 1s 83ms/step - loss: 0.7798\n",
            "Epoch 2/5\n",
            "8/8 [==============================] - 1s 80ms/step - loss: 0.6195\n",
            "Epoch 3/5\n",
            "8/8 [==============================] - 1s 79ms/step - loss: 0.3154\n",
            "Epoch 4/5\n",
            "8/8 [==============================] - 1s 80ms/step - loss: 0.1967\n",
            "Epoch 5/5\n",
            "8/8 [==============================] - 1s 80ms/step - loss: 0.0901\n",
            "Pretraining time: 3.98s\n",
            "Pretrained model saved to ./results/news/cnn/phase2/pretrained.h5\n",
            "\n",
            "### Phase 3: self-training ###\n",
            "Update interval: 50\n",
            "3988/3988 [==============================] - 21s 5ms/step\n",
            "\n",
            "Iter 0: Fraction of documents with label changes: 0.0 %\n",
            "\n",
            "Iter 50: Fraction of documents with label changes: 2.724 %\n",
            "\n",
            "Iter 100: Fraction of documents with label changes: 3.615 %\n",
            "\n",
            "Iter 150: Fraction of documents with label changes: 3.092 %\n",
            "\n",
            "Iter 200: Fraction of documents with label changes: 3.281 %\n",
            "\n",
            "Iter 250: Fraction of documents with label changes: 3.096 %\n",
            "\n",
            "Iter 300: Fraction of documents with label changes: 3.245 %\n",
            "\n",
            "Iter 350: Fraction of documents with label changes: 3.128 %\n",
            "\n",
            "Iter 400: Fraction of documents with label changes: 3.157 %\n",
            "\n",
            "Iter 450: Fraction of documents with label changes: 2.69 %\n",
            "\n",
            "Iter 500: Fraction of documents with label changes: 3.072 %\n",
            "\n",
            "Iter 550: Fraction of documents with label changes: 1.998 %\n",
            "\n",
            "Iter 600: Fraction of documents with label changes: 2.526 %\n",
            "\n",
            "Iter 650: Fraction of documents with label changes: 1.982 %\n",
            "\n",
            "Iter 700: Fraction of documents with label changes: 1.754 %\n",
            "\n",
            "Iter 750: Fraction of documents with label changes: 1.266 %\n",
            "\n",
            "Iter 800: Fraction of documents with label changes: 1.237 %\n",
            "\n",
            "Iter 850: Fraction of documents with label changes: 1.342 %\n",
            "\n",
            "Iter 900: Fraction of documents with label changes: 0.998 %\n",
            "\n",
            "Iter 950: Fraction of documents with label changes: 0.834 %\n",
            "\n",
            "Iter 1000: Fraction of documents with label changes: 0.778 %\n",
            "\n",
            "Iter 1050: Fraction of documents with label changes: 0.766 %\n",
            "\n",
            "Iter 1100: Fraction of documents with label changes: 0.957 %\n",
            "\n",
            "Iter 1150: Fraction of documents with label changes: 0.698 %\n",
            "\n",
            "Iter 1200: Fraction of documents with label changes: 0.664 %\n",
            "\n",
            "Iter 1250: Fraction of documents with label changes: 0.796 %\n",
            "\n",
            "Iter 1300: Fraction of documents with label changes: 0.697 %\n",
            "\n",
            "Iter 1350: Fraction of documents with label changes: 0.653 %\n",
            "\n",
            "Iter 1400: Fraction of documents with label changes: 0.796 %\n",
            "\n",
            "Iter 1450: Fraction of documents with label changes: 0.816 %\n",
            "\n",
            "Iter 1500: Fraction of documents with label changes: 0.484 %\n",
            "\n",
            "Iter 1550: Fraction of documents with label changes: 0.896 %\n",
            "\n",
            "Iter 1600: Fraction of documents with label changes: 0.806 %\n",
            "\n",
            "Iter 1650: Fraction of documents with label changes: 0.565 %\n",
            "\n",
            "Iter 1700: Fraction of documents with label changes: 0.607 %\n",
            "\n",
            "Iter 1750: Fraction of documents with label changes: 0.665 %\n",
            "\n",
            "Iter 1800: Fraction of documents with label changes: 0.455 %\n",
            "\n",
            "Iter 1850: Fraction of documents with label changes: 0.536 %\n",
            "\n",
            "Iter 1900: Fraction of documents with label changes: 0.607 %\n",
            "\n",
            "Iter 1950: Fraction of documents with label changes: 0.578 %\n",
            "\n",
            "Iter 2000: Fraction of documents with label changes: 0.337 %\n",
            "\n",
            "Iter 2050: Fraction of documents with label changes: 0.741 %\n",
            "\n",
            "Iter 2100: Fraction of documents with label changes: 0.564 %\n",
            "\n",
            "Iter 2150: Fraction of documents with label changes: 0.48 %\n",
            "\n",
            "Iter 2200: Fraction of documents with label changes: 0.407 %\n",
            "\n",
            "Iter 2250: Fraction of documents with label changes: 0.462 %\n",
            "\n",
            "Iter 2300: Fraction of documents with label changes: 0.37 %\n",
            "\n",
            "Iter 2350: Fraction of documents with label changes: 0.423 %\n",
            "\n",
            "Iter 2400: Fraction of documents with label changes: 0.456 %\n",
            "\n",
            "Iter 2450: Fraction of documents with label changes: 0.464 %\n",
            "\n",
            "Iter 2500: Fraction of documents with label changes: 0.262 %\n",
            "\n",
            "Iter 2550: Fraction of documents with label changes: 0.579 %\n",
            "\n",
            "Iter 2600: Fraction of documents with label changes: 0.425 %\n",
            "\n",
            "Iter 2650: Fraction of documents with label changes: 0.471 %\n",
            "\n",
            "Iter 2700: Fraction of documents with label changes: 0.266 %\n",
            "\n",
            "Iter 2750: Fraction of documents with label changes: 0.37 %\n",
            "\n",
            "Iter 2800: Fraction of documents with label changes: 0.271 %\n",
            "\n",
            "Iter 2850: Fraction of documents with label changes: 0.262 %\n",
            "\n",
            "Iter 2900: Fraction of documents with label changes: 0.415 %\n",
            "\n",
            "Iter 2950: Fraction of documents with label changes: 0.33 %\n",
            "\n",
            "Iter 3000: Fraction of documents with label changes: 0.205 %\n",
            "\n",
            "Iter 3050: Fraction of documents with label changes: 0.378 %\n",
            "\n",
            "Iter 3100: Fraction of documents with label changes: 0.388 %\n",
            "\n",
            "Iter 3150: Fraction of documents with label changes: 0.472 %\n",
            "\n",
            "Iter 3200: Fraction of documents with label changes: 0.197 %\n",
            "\n",
            "Iter 3250: Fraction of documents with label changes: 0.242 %\n",
            "\n",
            "Iter 3300: Fraction of documents with label changes: 0.235 %\n",
            "\n",
            "Iter 3350: Fraction of documents with label changes: 0.166 %\n",
            "\n",
            "Iter 3400: Fraction of documents with label changes: 0.324 %\n",
            "\n",
            "Iter 3450: Fraction of documents with label changes: 0.263 %\n",
            "\n",
            "Iter 3500: Fraction of documents with label changes: 0.229 %\n",
            "\n",
            "Iter 3550: Fraction of documents with label changes: 0.306 %\n",
            "\n",
            "Iter 3600: Fraction of documents with label changes: 0.29 %\n",
            "\n",
            "Iter 3650: Fraction of documents with label changes: 0.364 %\n",
            "\n",
            "Iter 3700: Fraction of documents with label changes: 0.168 %\n",
            "\n",
            "Iter 3750: Fraction of documents with label changes: 0.102 %\n",
            "\n",
            "Iter 3800: Fraction of documents with label changes: 0.205 %\n",
            "\n",
            "Iter 3850: Fraction of documents with label changes: 0.154 %\n",
            "\n",
            "Iter 3900: Fraction of documents with label changes: 0.23 %\n",
            "\n",
            "Iter 3950: Fraction of documents with label changes: 0.227 %\n",
            "\n",
            "Iter 4000: Fraction of documents with label changes: 0.216 %\n",
            "\n",
            "Iter 4050: Fraction of documents with label changes: 0.268 %\n",
            "\n",
            "Iter 4100: Fraction of documents with label changes: 0.26 %\n",
            "\n",
            "Iter 4150: Fraction of documents with label changes: 0.341 %\n",
            "\n",
            "Iter 4200: Fraction of documents with label changes: 0.211 %\n",
            "\n",
            "Iter 4250: Fraction of documents with label changes: 0.137 %\n",
            "\n",
            "Iter 4300: Fraction of documents with label changes: 0.194 %\n",
            "\n",
            "Iter 4350: Fraction of documents with label changes: 0.157 %\n",
            "\n",
            "Iter 4400: Fraction of documents with label changes: 0.152 %\n",
            "\n",
            "Iter 4450: Fraction of documents with label changes: 0.212 %\n",
            "\n",
            "Iter 4500: Fraction of documents with label changes: 0.192 %\n",
            "\n",
            "Iter 4550: Fraction of documents with label changes: 0.254 %\n",
            "\n",
            "Iter 4600: Fraction of documents with label changes: 0.239 %\n",
            "\n",
            "Iter 4650: Fraction of documents with label changes: 0.354 %\n",
            "\n",
            "Iter 4700: Fraction of documents with label changes: 0.176 %\n",
            "\n",
            "Iter 4750: Fraction of documents with label changes: 0.142 %\n",
            "\n",
            "Iter 4800: Fraction of documents with label changes: 0.138 %\n",
            "\n",
            "Iter 4850: Fraction of documents with label changes: 0.181 %\n",
            "\n",
            "Iter 4900: Fraction of documents with label changes: 0.121 %\n",
            "\n",
            "Iter 4950: Fraction of documents with label changes: 0.196 %\n",
            "\n",
            "Iter 5000: Fraction of documents with label changes: 0.142 %\n",
            "\n",
            "Iter 5050: Fraction of documents with label changes: 0.208 %\n",
            "\n",
            "Iter 5100: Fraction of documents with label changes: 0.237 %\n",
            "\n",
            "Iter 5150: Fraction of documents with label changes: 0.324 %\n",
            "\n",
            "Iter 5200: Fraction of documents with label changes: 0.149 %\n",
            "\n",
            "Iter 5250: Fraction of documents with label changes: 0.149 %\n",
            "\n",
            "Iter 5300: Fraction of documents with label changes: 0.111 %\n",
            "\n",
            "Iter 5350: Fraction of documents with label changes: 0.233 %\n",
            "\n",
            "Iter 5400: Fraction of documents with label changes: 0.11 %\n",
            "\n",
            "Iter 5450: Fraction of documents with label changes: 0.172 %\n",
            "\n",
            "Iter 5500: Fraction of documents with label changes: 0.121 %\n",
            "\n",
            "Iter 5550: Fraction of documents with label changes: 0.17 %\n",
            "\n",
            "Iter 5600: Fraction of documents with label changes: 0.266 %\n",
            "\n",
            "Iter 5650: Fraction of documents with label changes: 0.297 %\n",
            "\n",
            "Iter 5700: Fraction of documents with label changes: 0.097 %\n",
            "\n",
            "Fraction: 0.097 % < tol: 0.1 %\n",
            "Reached tolerance threshold. Stopping training.\n",
            "Final model saved to: ./results/news/cnn/phase3/final.h5\n",
            "Self-training time: 4104.74s\n",
            "\n",
            "### Generating outputs ###\n",
            "Classification results are written in ./news/out.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import KeyedVectors\n",
        "\n",
        "# Step 1: Read your embeddings\n",
        "embedding_file = \"/content/drive/MyDrive/CS512_Assignment/CatE/datasets/news/emb_news_category_word.txt\"\n",
        "\n",
        "# Step 2: Create a KeyedVectors instance\n",
        "word_vectors = KeyedVectors(vector_size=100)\n",
        "\n",
        "with open(embedding_file, 'r') as f:\n",
        "  first_line = True\n",
        "  for line in f:\n",
        "    if first_line: first_line=False; continue\n",
        "    parts = line.split()\n",
        "    word = parts[0]\n",
        "    vector = np.array([float(x) for x in parts[1:101]], dtype=np.float32)\n",
        "    #print(word)\n",
        "    #print(vector)\n",
        "\n",
        "    # Step 3: Populate KeyedVectors\n",
        "    word_vectors.add_vector(word, vector)\n",
        "\n",
        "# Step 4: Save in Word2Vec format\n",
        "word_vectors.save_word2vec_format(\"emb_news_category_word_word2vec_format\")\n"
      ],
      "metadata": {
        "id": "tRYk0OX5Av2v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#with CatE embeddings\n",
        "%cd /content/drive/MyDrive/CS512_Assignment/WeSTClass/\n",
        "!python main.py --dataset 'news' --sup_source 'keywords' --model 'cnn' --with_evaluation 'False' --maxiter 50000 --pretrain_epochs 5"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SjiLGv6MDkVZ",
        "outputId": "ca2757f7-ddfc-4a1b-8f06-c4d2190ce974"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/CS512_Assignment/WeSTClass\n",
            "2023-10-28 02:35:22.249696: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-10-28 02:35:22.249768: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-10-28 02:35:22.249965: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-10-28 02:35:22.259425: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-10-28 02:35:23.584422: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Namespace(dataset='news', model='cnn', sup_source='keywords', with_evaluation='False', batch_size=256, maxiter=50000, pretrain_epochs=5, update_interval=None, alpha=0.2, beta=500, gamma=50, delta=0.1, trained_weights=None)\n",
            "\n",
            "### Dataset statistics: ###\n",
            "Document max length: 202 (words)\n",
            "Document average length: 41.275188087774296 (words)\n",
            "Document length std: 12.11052194316095 (words)\n",
            "Defined maximum document length: 100 (words)\n",
            "Fraction of truncated documents: 0.003252351097178683\n",
            "Vocabulary Size: 106496\n",
            "\n",
            "### Supervision type: Class-related Keywords ###\n",
            "Keywords for each class: \n",
            "Supervision content of class 0:\n",
            "['liberalism', 'agendas', 'political', 'undemocratic', 'fundamentalist', 'polarizing', 'ideological', 'misunderstood', 'adolf_hitler', 'extremism']\n",
            "Supervision content of class 1:\n",
            "['network', 'team_report_-_august', 'andreychuk', 'sportsnetwork_game_preview', 'ticker', 'tx', 'ny_jets', 'sportsticker', 'team_report_-_december', 'fl']\n",
            "Supervision content of class 2:\n",
            "['personal-computer', 'pc-making', 'wealth-management', 'systemcorp', 'customer', 'lenovo', 'majority_stake', 'wealth_management', '1.17', 'ubs_ag']\n",
            "Supervision content of class 3:\n",
            "['commercializing', 'touch_screen', 'internet_protocol_version_6', 'graphics_processing_unit', 'technologies', 'ieee', 'second-generation', 'low-power', '65-nanometer', 'proof-of-concept']\n",
            "\n",
            "### Input preparation ###\n",
            "Loading existing Word2Vec model ./news/embedding...\n",
            "/usr/local/lib/python3.10/dist-packages/keras/src/initializers/initializers.py:120: UserWarning: The initializer RandomUniform is unseeded and being called multiple times, which will return identical values each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initializer instance more than once.\n",
            "  warnings.warn(\n",
            "\n",
            "### Phase 1: vMF distribution fitting & pseudo document generation ###\n",
            "Retrieving top-t nearest words...\n",
            "Final expansion size t = 210\n",
            "Top-t nearest words for each class:\n",
            "Class 0:\n",
            "['liberalism', 'agendas', 'fundamentalist', 'political', 'undemocratic', 'adolf_hitler', 'ideological', 'misunderstood', 'ideology', 'extremism', 'polarizing', 'politics', 'philosophical', 'secular', 'homosexuality', 'patriotism', 'democracy', 'populist', 'mistrust', 'religion', 'pacifism', 'sentiments', 'reformists', 'liberal', 'one-party', 'hatred', 'rightist', 'anarchists', 'conservative', 'communism', 'democratic', 'repressive', 'combatting', 'imbecile', 'oppression', 'self-determination', 'free_market', 'advocacy', 'religious', 'fomenting', 'liberal_party', 'reformist', 'creationism', 'regimes', 'staunchly', 'grievances', 'beliefs', 'conservatives', 'election-eve', 'divisive', 'stage-managed', 'centre-right', 'misunderstanding', 'pragmatic', 'challenger_john_kerry', 'confrontational', 'factional', 'morality', 'disaffected', 'mocking', 'michael_howard', 'traitor', 'doubting', 'like-minded', 'doctrine', 'fundamentalists', 'extremists', 'politicizing', 'ambivalence', 'resentment', 'capitalism', 'muslim_world', 'blasphemy', 'anti-israeli', 'viewpoints', 'animosity', 'reelected', 'yitzhak_rabin', 'emphatically', 'democrat', 'national_party', 'grassroots', 'terrorism', 'hard-liners', 'decried', 'orange_revolution', 'hardliners', 'unfit', 'intellectuals', 'non-violent', 'misgivings', 'world_peace', 'sectarian', 'presidency', 'unity', 'contradictions', 'social', 'discredited', 'jesse_jackson', 'economic_policy', 'freedoms', 'shameful', 'friendships', 'right-leaning', 'anti-abortion', 'leadership', 'disillusioned', 'nominating', 'national_security', 'fervent', 'caucus', 'ex-soviet', 'human-rights', 'religions', 'ukrainian_president', 'critique', 'nationalist', 'authoritarian', 'gays', 'unwillingness', 'judiciary', 'right-wing', 'john_kerry', 'appointees', 'persecution', 'hypocrisy', 'politician', 'strong-dollar', 'opposition', 'tom_delay', 'disquiet', 'holy_war', 'batasuna', 'neo-nazi', 'starkly', 'howard_dean', 'far-right', 'distorted', 'wavering', 'vilified', 'boycotting', 'long-held', 'instigating', 'democratic_party', 'republicans', 'insurgency', 'iraq_war', 'anglican_church', 'embolden', 'rhetoric', 'denouncing', 'asserting', 'christians', 'politicians', 'left-leaning', 'manifesto', 'unwavering', 'zeal', 'far-rightists', 'antiwar', 'anxieties', 'stephen_harper', 'reconciliation', 'clergy', 'stronger_ties', 'contradictory', 'uncompromising', 'democrats', 'jean_bertrand_aristide', 'foreign_policy', 'labor_party', 'homosexuals', 'gay_rights', 'presidential_runoff', 'overthrew', 'anti-us', 'moral', 'presidential_race', 'economic', 'unethical', 'moscow-backed', 'contradiction', 'bible', 'protege', 'gay_marriage', 'aung_san', 'sermons', 'radical_islam', 'criticism', 'commander_in_chief', 'honesty', 'movement', 'abortion', 'polls', 'ideals', 'political_parties', 'selfish', 'center-left', 'stalinist', 'street_protests', 'democratization', 'tyrant', 'children_overboard', 'status_quo', 'endangering', 'reform', 'tabare', 'diplomacy', 'sin', 'motives']\n",
            "Class 1:\n",
            "['team_report_-_august', 'sportsnetwork_game_preview', 'sports', 'ticker', 'team_report_-_december', 'andreychuk', 'network', 'tx', 'team_report_-_november', 'fl', 'ny_jets', 'sportsticker', 'team_report_-_september', 'tropicana_field', 'jeff_fassero', '1510', 'activated_guard', 'american_airlines_center', 'nashville_predators', 'minnesota_wild', 'u-wire', 're-signs', '122-113', 'lhp', '91-86', 'luther_head', 'dajuan_wagner', 'wi', '85-51', 'b.j', 'carlos_arroyo', 'mccoy', 'gonzaga', '15-10', 'ben_gordon', 'englewood', 'greenville', '20-14', '16-4', 'nicholls_state', '83-66', 'taquan', 'wachovia_center', 'zydrunas_ilgauskas', 'sandy_alomar_jr', 'chris_carpenter', '21-6', '117-98', 'stillwater', '106-83', 'wwzn', 'brazelton', 'kenny_rogers', 'mustafa_shakur', 'tinsley', 'mark_redman', '92-86', 'continental_airlines_arena', '74-37', 'jarrod_washburn', 'kevin_willis', 'west-leading', 'ny_giants', 'university_of_south_carolina', '107-101', 'john_flaherty', 'warrick', 'mike_lowell', '12-8', 'mike_maroth', 'mo.', 'julian_peterson', 'best-of-three', '3-7', 'si.com', 'drew_gooden', 'dustan', 'berea', 'joey_graham', 'morten_andersen', 'game-high', 'wade_boggs', 'todd_walker', 'mi', 'clobber', '117-101', 'houston_bowl', '121-99', 'tn', 'pro_baseball', 'woodson', 'ritchie', 'bob_wickman', 'antonio_mcdyess', 'frank_gore', '699th', \"d'backs\", 'wendell', 'ct', 'brian_griese', 'julius_hodge', '55-28', 'adam_laroche', 'detroit_red_wings', 'conner', 'los_angeles_clippers', '92-76', '92-69', 'il', '7-4', 'billups', 'outguns', 'jacksonville', 'ginter', 'juan_uribe', '107-102', 'raymond_felton', 'reliant_stadium', 'doubleheaders', 'signed_free_agent', 'chicago_blackhawks', 'signed_free-agent', 'comerica_park', 'pepsi_center', '38-13', 'ohio_university', 'weei', '96-90', 'terry_mulholland', 'steve_smith', 'pcl', 'rapids', 'arthur_rhodes', 'nallen', '107-100', 'dwayne', 'uncasville', '113-94', 'team-high', 'jon_barry', 'mantei', 'miami-florida', 'one-game', 'dorsey_levens', '90-86', 'trent', 'drubs', 'pat_garrity', 'injures_knee', '19-17', '31-yard_field_goal', 'az', 'hairston', 'st_louis', 'cleveland_cavaliers', 'elton_brand', 'jacobsen', 'sacramento_monarchs', 'weber_state', '16-6', 'trent_green', 'white_plains', 'a_10-3', 'ga', 'eden_prairie', 'tenn', 'lake_forest', '35-yard_field_goal', 'edt', 'homestand', 'josh_beckett', '76-67', 'neil_rackers', 'voshon_lenard', '10-3', 'ronald_curry', 'brian_urlacher', 'jarrett_jack', 'tampa_bay_devil_rays', '30-28', '32-29', 'ut', '43-39', '114-109', 'umass-dartmouth', 'inf', 'baron_davis', '81-69', 'diana_taurasi', 'alfonso_soriano', 'matt_birk', 'kansas_city', 'blackhawks', 'defensive_tackle', 'lefors', 'americanairlines', 'jeff_george', 'anaheim_angels', 'a_10-7', 'rhodes', '4-7', 'ir', '10-2', 'hattiesburg', 'nick_swisher', 'cory_lidle', 'winless', 'billy_reay', 'minnesota_vikings', 'griese']\n",
            "Class 2:\n",
            "['pc-making', 'wealth-management', 'personal-computer', 'systemcorp', 'business', 'ubs_ag', '1.17', 'majority_stake', 'lenovo', 'dynamicsoft', 'cinven', 'wealth_management', '3.05', 'mercantile', '1.03', 'cable-television', '1.53', 'tenovis', 'project_finance', 'toronto_dominion_bank', 'commercial_bank', 'dial-up_internet_access', 'sinotrans', 'kvault', 'mass.-based', 'franchisor', 'non-memory', 'china-based', 'acquisition', 'gullivers', 'canadian_imperial_bank_of_commerce', 'asset_management', 'venetica', 'unit', 'backup_software', 'customer', 'investcorp', 'cerberus', 'project_management', '1.48', '1.25b', '1.5b', 'entergy-koch', 'personal_computing', 'isuzu_motors', 'content_integration', 'privately-held', 'company', '1.84bn', 'medquist', '1.25', 'electric_utility', '430m', 'buy', 'purchase', 'ready-mixed', 'henkel', '493', 'home-loan', 'ibm_pc', '1.27bn', 'charles_schwab', 'centene', 'menlo_worldwide', 'buys', 'unprofitable', 'bancshares', 'photomasks', 'atlas_copco', '11.5', 'scotch_whisky', 'cerberus_capital_management', 'kvault_software', 'banknorth', 'iesi', 'parker_hannifin', 'frango', 'sprint_corp.', 'blackstone_group', 'davita', 'cigna', 'ny_times', 'acquire', 'information-technology', 'ionics', 'borgwarner', 'wheeling-pittsburgh', 'barnes_noble', 'westlb', 'fourth-largest', '11i', 'rexel', 'sprint_corporation', 'ameritrade', 'billion-euro', 'jack_daniel', 'global_exchange', 'lnr_property', 'paykel', 'back-office', 'thomson_corp.', 'winterthur', 'diversified', 'apax', 'blackrock', 'buyer', 'rmc', 'corp.', 'gambro_ab', '1.27', 'second-largest', '590', 'business_jet', 'ge_capital', 'confectionery', '13bn', 'equiserve', '14.9', 'austin-based', 'royal_doulton', 'assets', 'barrick_gold', 'noranda', 'water_treatment', 'discount_brokerage', 'lojack', 'managed_services', 'logicalis', 'controlling_stake', 'international_business_machines', '1.9bn', 'netherlands-based', 'cox_enterprises', '10.8', 'privately_held_company', 'sevenspace', 'infonet_services', 'vendor', 'private_equity_firm', 'lifecycle_management', 'sell', 'duelguide', 'net6', 'arvinmeritor', 'second-largest_bank', 'iron_mountain', 'self-service', 'mts', 'dantz_development', '965', 'jersey-based', '463', 'banco_bilbao_vizcaya_argentaria', 'edocs', 'banknorth_group_inc', 'bank', '913', 'stanchart', 'fixed-income', 'mci_inc.', 'medex', 'writedowns', 'headquartered', 'veritas_software', 'chalone', 'canwest_global', 'napa_valley', '1.92', 'soundview', '579', 'ingersoll-rand', 'eserver_i5', 'wholesaler', 'e-trade', 'lender', 'cibc', 'waste_management', 'bancgroup', 'china_resources', 'warburg_pincus', '1.14', 'corp', 'cendant', 'cooper_tire', 'dlj', 'deutsche_bank_ag', 'mobilesoft', 'mid-priced', 'glg', 'gasunie', 'altoids', 'recapitalization', 'netsolve', 'acquires', 'citrix', 'kidney-care', 'fifth-largest', 'provider', 'reinsurance', 'investments', 'private_equity', 'aviva', '87.5', 'sell_stake', '415m', 'tech_pacific', '293', 'sportsco', 'cox_communications', 'natl']\n",
            "Class 3:\n",
            "['internet_protocol_version_6', 'graphics_processing_unit', 'commercializing', 'low-power', '65-nanometer', 'technology', 'second-generation', 'touch_screen', 'ieee', 'compliant', 'hypertransport', 'proof-of-concept', 'technologies', 'lcos', 'multi-core', 'powernow', 'jointly_developed', 'chipsets', 'code_division_multiple_access', '4500', 'front-side_bus', 'turbocache', 'wavesat', 'near-field', 'standards-based', 'k8', 'gpu', 'ofdm', '90-nanometer', 'cdma', 'interconnect', 'next-generation', 'heterogeneous', 'smart_card', 'hard-disk_drives', 'nec_corporation', 'hard_disk_drives', '7710', 'p5', 'end-users', 'integrator', 'iscsi', 'co-developed', 'pay-per-use', 'hd-dvds', 'strained_silicon', 'specification', 'emulex', 'nanometer', 'processor', 'graphics_engine', 'ipv6', 'data-centric', 'commercialize', '512mbit', 'interfaces', 'pc-based', 'germanium', 'caymas', 'powerpc', 'crs-1', 'suretype', 'nforce', 'standard', 'utilizing', 'chips', 'freescale', 'sram', 'end-to-end', 'specs', 'hsdpa', 'ultra-thin', '700gr', 'storage_technologies', 'c-series', 'visualization', 'architecture', 'serial_ata', 'perpendicular', 'toolset', 'nanometre', 'sempron', 'accelerator', 'workstation', 'processors', 'uwb', 'commercialization', 'rackable', '90nm', 'smes', 'wlan', 'developed', 'rack-mounted', 'power-based', 'power_management', 'develops', 'easy-to-use', '7270', 'modular', 'wcdma', 'airgo', 'storage_platform', 'graphics_processor', 'incorporating', 'in-vehicle', 'prototypes', 'hgst', 'develop', 'video_codec', 'chip', 'adaptable', 'devices', 'intel-based', '7g', 'wimax', 'apis', 'gprs', 'intels', 'cores', 'microprocessor', 'flash-ofdm', 'transistors', 'multicast', 'grid_computing', 'dual-layer', 'silicon_carbide', 'dual-format', 'high-availability', 'peripherals', 'pay-for-use', 'memory-tech', 'transistor', 'netweaver', 'based', 'lans', 'interface', 'display_panels', 'grid-computing', 'opteron', 'zigbee', 'cpus', 'mobile', 'linux-only', 'dual-band', 'web-services', 'interoperable', 'add-in', 'nas', 'integrated', 'btx', '32-bit', 'hard-disk_drive', 'gigaset', '65nm', 'wavelink', 'specifications', 'amd64', 'rechargeable', 'codecs', 'front-end', 'fibre_channel', 'flash_memory', '6800', '802', 'computing_platform', 'optical', 'desktop-replacement', 'totalstorage', 'dmb', 'prototype', '6200', 'communication', 'polycom', 'set-top', 'tri-band', 'scalability', 'objectweb', 'hpc', '128mb', '64-way', 'sdk', 'data_storage', 'atomic_clock', 'developing', 'scsi', 'notebook_pcs', 'amd', 'enterprise_search', 'mercury_interactive', '4000', 'hypermemory', 'sch-s250', 'connector', 'itanium-based', 'dynamic_systems', '8gb', 'amds', 'layered', 'graphics_card', 'co-develop', 'hdd', 'silicon', 'fx-55', 'linux_standard_base', 'development', 'pathscale', 'resell', 'lsb', 'compatible', 'network-attached_storage']\n",
            "Finished vMF distribution fitting.\n",
            "Pseudo documents generation...\n",
            "Finished Pseudo documents generation.\n",
            "\n",
            "### Phase 2: pre-training with pseudo documents ###\n",
            "/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/legacy/gradient_descent.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n",
            "\n",
            "Neural model summary: \n",
            "Model: \"classifier\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input (InputLayer)          [(None, 100)]                0         []                            \n",
            "                                                                                                  \n",
            " embedding (Embedding)       (None, 100, 100)             1064960   ['input[0][0]']               \n",
            "                                                          0                                       \n",
            "                                                                                                  \n",
            " conv1d (Conv1D)             (None, 99, 20)               4020      ['embedding[0][0]']           \n",
            "                                                                                                  \n",
            " conv1d_1 (Conv1D)           (None, 98, 20)               6020      ['embedding[0][0]']           \n",
            "                                                                                                  \n",
            " conv1d_2 (Conv1D)           (None, 97, 20)               8020      ['embedding[0][0]']           \n",
            "                                                                                                  \n",
            " conv1d_3 (Conv1D)           (None, 96, 20)               10020     ['embedding[0][0]']           \n",
            "                                                                                                  \n",
            " global_max_pooling1d (Glob  (None, 20)                   0         ['conv1d[0][0]']              \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_1 (Gl  (None, 20)                   0         ['conv1d_1[0][0]']            \n",
            " obalMaxPooling1D)                                                                                \n",
            "                                                                                                  \n",
            " global_max_pooling1d_2 (Gl  (None, 20)                   0         ['conv1d_2[0][0]']            \n",
            " obalMaxPooling1D)                                                                                \n",
            "                                                                                                  \n",
            " global_max_pooling1d_3 (Gl  (None, 20)                   0         ['conv1d_3[0][0]']            \n",
            " obalMaxPooling1D)                                                                                \n",
            "                                                                                                  \n",
            " concatenate (Concatenate)   (None, 80)                   0         ['global_max_pooling1d[0][0]',\n",
            "                                                                     'global_max_pooling1d_1[0][0]\n",
            "                                                                    ',                            \n",
            "                                                                     'global_max_pooling1d_2[0][0]\n",
            "                                                                    ',                            \n",
            "                                                                     'global_max_pooling1d_3[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " dense (Dense)               (None, 20)                   1620      ['concatenate[0][0]']         \n",
            "                                                                                                  \n",
            " dense_1 (Dense)             (None, 4)                    84        ['dense[0][0]']               \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 10679384 (40.74 MB)\n",
            "Trainable params: 29784 (116.34 KB)\n",
            "Non-trainable params: 10649600 (40.62 MB)\n",
            "__________________________________________________________________________________________________\n",
            "\n",
            "Pretraining...\n",
            "Epoch 1/5\n",
            "8/8 [==============================] - 2s 141ms/step - loss: 0.4406\n",
            "Epoch 2/5\n",
            "8/8 [==============================] - 1s 142ms/step - loss: 0.0970\n",
            "Epoch 3/5\n",
            "8/8 [==============================] - 1s 144ms/step - loss: 0.0432\n",
            "Epoch 4/5\n",
            "8/8 [==============================] - 1s 141ms/step - loss: 0.0232\n",
            "Epoch 5/5\n",
            "8/8 [==============================] - 1s 142ms/step - loss: 0.0132\n",
            "Pretraining time: 6.73s\n",
            "Pretrained model saved to ./results/news/cnn/phase2/pretrained.h5\n",
            "\n",
            "### Phase 3: self-training ###\n",
            "Update interval: 50\n",
            "3988/3988 [==============================] - 38s 9ms/step\n",
            "\n",
            "Iter 0: Fraction of documents with label changes: 0.0 %\n",
            "\n",
            "Iter 50: Fraction of documents with label changes: 2.952 %\n",
            "\n",
            "Iter 100: Fraction of documents with label changes: 2.746 %\n",
            "\n",
            "Iter 150: Fraction of documents with label changes: 1.813 %\n",
            "\n",
            "Iter 200: Fraction of documents with label changes: 1.498 %\n",
            "\n",
            "Iter 250: Fraction of documents with label changes: 1.204 %\n",
            "\n",
            "Iter 300: Fraction of documents with label changes: 1.068 %\n",
            "\n",
            "Iter 350: Fraction of documents with label changes: 1.393 %\n",
            "\n",
            "Iter 400: Fraction of documents with label changes: 0.922 %\n",
            "\n",
            "Iter 450: Fraction of documents with label changes: 0.774 %\n",
            "\n",
            "Iter 500: Fraction of documents with label changes: 0.882 %\n",
            "\n",
            "Iter 550: Fraction of documents with label changes: 0.838 %\n",
            "\n",
            "Iter 600: Fraction of documents with label changes: 0.567 %\n",
            "\n",
            "Iter 650: Fraction of documents with label changes: 0.567 %\n",
            "\n",
            "Iter 700: Fraction of documents with label changes: 0.699 %\n",
            "\n",
            "Iter 750: Fraction of documents with label changes: 0.4 %\n",
            "\n",
            "Iter 800: Fraction of documents with label changes: 0.389 %\n",
            "\n",
            "Iter 850: Fraction of documents with label changes: 0.685 %\n",
            "\n",
            "Iter 900: Fraction of documents with label changes: 0.474 %\n",
            "\n",
            "Iter 950: Fraction of documents with label changes: 0.398 %\n",
            "\n",
            "Iter 1000: Fraction of documents with label changes: 0.466 %\n",
            "\n",
            "Iter 1050: Fraction of documents with label changes: 0.372 %\n",
            "\n",
            "Iter 1100: Fraction of documents with label changes: 0.276 %\n",
            "\n",
            "Iter 1150: Fraction of documents with label changes: 0.321 %\n",
            "\n",
            "Iter 1200: Fraction of documents with label changes: 0.56 %\n",
            "\n",
            "Iter 1250: Fraction of documents with label changes: 0.3 %\n",
            "\n",
            "Iter 1300: Fraction of documents with label changes: 0.213 %\n",
            "\n",
            "Iter 1350: Fraction of documents with label changes: 0.333 %\n",
            "\n",
            "Iter 1400: Fraction of documents with label changes: 0.277 %\n",
            "\n",
            "Iter 1450: Fraction of documents with label changes: 0.246 %\n",
            "\n",
            "Iter 1500: Fraction of documents with label changes: 0.23 %\n",
            "\n",
            "Iter 1550: Fraction of documents with label changes: 0.187 %\n",
            "\n",
            "Iter 1600: Fraction of documents with label changes: 0.178 %\n",
            "\n",
            "Iter 1650: Fraction of documents with label changes: 0.213 %\n",
            "\n",
            "Iter 1700: Fraction of documents with label changes: 0.337 %\n",
            "\n",
            "Iter 1750: Fraction of documents with label changes: 0.205 %\n",
            "\n",
            "Iter 1800: Fraction of documents with label changes: 0.206 %\n",
            "\n",
            "Iter 1850: Fraction of documents with label changes: 0.178 %\n",
            "\n",
            "Iter 1900: Fraction of documents with label changes: 0.19 %\n",
            "\n",
            "Iter 1950: Fraction of documents with label changes: 0.211 %\n",
            "\n",
            "Iter 2000: Fraction of documents with label changes: 0.121 %\n",
            "\n",
            "Iter 2050: Fraction of documents with label changes: 0.157 %\n",
            "\n",
            "Iter 2100: Fraction of documents with label changes: 0.176 %\n",
            "\n",
            "Iter 2150: Fraction of documents with label changes: 0.164 %\n",
            "\n",
            "Iter 2200: Fraction of documents with label changes: 0.249 %\n",
            "\n",
            "Iter 2250: Fraction of documents with label changes: 0.149 %\n",
            "\n",
            "Iter 2300: Fraction of documents with label changes: 0.148 %\n",
            "\n",
            "Iter 2350: Fraction of documents with label changes: 0.158 %\n",
            "\n",
            "Iter 2400: Fraction of documents with label changes: 0.215 %\n",
            "\n",
            "Iter 2450: Fraction of documents with label changes: 0.186 %\n",
            "\n",
            "Iter 2500: Fraction of documents with label changes: 0.089 %\n",
            "\n",
            "Fraction: 0.089 % < tol: 0.1 %\n",
            "Reached tolerance threshold. Stopping training.\n",
            "Final model saved to: ./results/news/cnn/phase3/final.h5\n",
            "Self-training time: 2368.49s\n",
            "\n",
            "### Generating outputs ###\n",
            "Classification results are written in ./news/out.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MbCG9AE7FXcz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Validation"
      ],
      "metadata": {
        "id": "JqdATVvvwRBX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "true_labels = []\n",
        "pred_labels = []\n",
        "with open(\"/content/drive/MyDrive/CS512_Assignment/assignment/news_train_labels.txt\", 'r') as truth:\n",
        "  count = 0\n",
        "  for line in truth:\n",
        "    if count == 100: break\n",
        "    true_labels.append(int(line))\n",
        "with open(\"/content/drive/MyDrive/CS512_Assignment/WeSTClass/news/out.txt\", 'r') as pred:\n",
        "  count = 0\n",
        "  for line in pred:\n",
        "    if count == 100: break\n",
        "    pred_labels.append(int(line))\n",
        "true_arr = np.array(true_labels[:100])\n",
        "pred_arr = np.array(pred_labels[:100])\n",
        "\n",
        "from sklearn.metrics import f1_score\n",
        "f1 = f1_score(true_arr, pred_arr, average='micro')\n",
        "print(f1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TCbR84VvwQpq",
        "outputId": "c1c9437e-3847-4610-f239-fb993c5ee540"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.82\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "true_labels = []\n",
        "pred_labels = []\n",
        "with open(\"/content/drive/MyDrive/CS512_Assignment/assignment/movies_train_labels.txt\", 'r') as truth:\n",
        "  count = 0\n",
        "  for line in truth:\n",
        "    if count == 100: break\n",
        "    true_labels.append(int(line))\n",
        "with open(\"/content/drive/MyDrive/CS512_Assignment/WeSTClass/movies/out.txt\", 'r') as pred:\n",
        "  count = 0\n",
        "  for line in pred:\n",
        "    if count == 100: break\n",
        "    pred_labels.append(int(line))\n",
        "true_arr = np.array(true_labels[:100])\n",
        "pred_arr = np.array(pred_labels[:100])\n",
        "\n",
        "from sklearn.metrics import f1_score\n",
        "f1 = f1_score(true_arr, pred_arr, average='micro')\n",
        "print(f1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lGnljDw9yKCw",
        "outputId": "4be90e22-071e-4f56-cb18-5b4ffdb55a76"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.75\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cnN0LiNvGgaj",
        "outputId": "add49ece-8746-4bc3-d042-73eeca220bf3"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai\n",
            "  Downloading openai-0.28.1-py3-none-any.whl (76 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/77.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m71.7/77.0 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.0/77.0 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai) (3.8.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (3.3.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2023.7.22)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.3.1)\n",
            "Installing collected packages: openai\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\n",
            "llmx 0.0.15a0 requires tiktoken, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed openai-0.28.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 4: With LLMs (GPT 3.5)"
      ],
      "metadata": {
        "id": "ebMpiJ9iqDoX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install openai\n",
        "import os\n",
        "import openai\n",
        "os.environ['OPENAI_API_KEY'] = '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'\n",
        "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "#openai.Model.list()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DE36qXZ_Hbyc",
        "outputId": "00d645cc-c4b1-4446-9743-6d666e6f135d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai\n",
            "  Downloading openai-0.28.1-py3-none-any.whl (76 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.0/77.0 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai) (3.8.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (3.3.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2023.7.22)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.3.1)\n",
            "Installing collected packages: openai\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\n",
            "llmx 0.0.15a0 requires tiktoken, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed openai-0.28.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def chatWithGPT_movies(docs):\n",
        "  prompt = \"Categorize this document from a movie review dataset as either one of - *bad* or *good*. Choose one (the most likely) of the 2 labels I provided the document/review. Pay particular attention to the core idea in the document for deciding which class it belongs to. Return only the class name, i.e. good or bad. Also note that everything not separated by a newline character is one review/document: \"\n",
        "  prompt = prompt + docs\n",
        "  completion = openai.ChatCompletion.create(\n",
        "  model=\"gpt-4\",\n",
        "  messages=[\n",
        "  {\"role\": \"user\", \"content\": prompt}\n",
        "  ]\n",
        "  )\n",
        "  return completion.choices[0].message.content"
      ],
      "metadata": {
        "id": "cex0I8hjK-_g"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def chatWithGPT_news(docs):\n",
        "  prompt = \"Categorize this document from a news article dataset as either one of these classes - *Business* or *Technology* or *Sports* or *Politics*. Choose one (the most likely) of the 4 labels I provided the document/review. Pay particular attention to the core idea in the document for deciding which class it belongs to. Return only the class name, i.e. *Business* or *Technology* or *Sports* or *Politics*. Also note that everything not separated by a newline character is one news article/document: \"\n",
        "  prompt = prompt + docs\n",
        "  completion = openai.ChatCompletion.create(\n",
        "  model= \"gpt-4\",  #\"gpt-3.5-turbo\",\n",
        "  messages=[\n",
        "  {\"role\": \"user\", \"content\": prompt}\n",
        "  ]\n",
        "  )\n",
        "  return completion.choices[0].message.content"
      ],
      "metadata": {
        "id": "khhFRzLvqI9s"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"/content/drive/MyDrive/CS512_Assignment/assignment/movies_train.txt\", 'r') as file:\n",
        "  count = 0\n",
        "  with open('/content/drive/MyDrive/CS512_Assignment/movies_GPT_preds.txt', 'w') as write_file:\n",
        "    for line in file:\n",
        "      if count > 100:\n",
        "        break\n",
        "      if count > -1:\n",
        "        result = chatWithGPT_movies(line)\n",
        "        print(f\"Iteration{count}: \", result)\n",
        "        write_file.write(result + '\\n')\n",
        "      count+=1"
      ],
      "metadata": {
        "id": "WbW2EtZIO6HS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "true_labels = []\n",
        "pred_labels = []\n",
        "with open(\"/content/drive/MyDrive/CS512_Assignment/assignment/movies_train_labels.txt\", 'r') as truth:\n",
        "  count = 0\n",
        "  for line in truth:\n",
        "    if count == 100: break\n",
        "    true_labels.append(int(line))\n",
        "with open(\"/content/drive/MyDrive/CS512_Assignment/movies_GPT_preds.txt\", 'r') as pred:\n",
        "  count = 0\n",
        "  for line in pred:\n",
        "    if count == 100: break\n",
        "    if line == \"good\\n\" or line == \"Good\\n\":\n",
        "      pred_labels.append(1)\n",
        "    elif line == \"bad\\n\" or line == \"Bad\\n\":\n",
        "      pred_labels.append(0)\n",
        "    else:\n",
        "      print(line)\n",
        "      pred_labels.append(2)\n",
        "true_arr = np.array(true_labels[:100])\n",
        "pred_arr = np.array(pred_labels[:100])\n",
        "\n",
        "from sklearn.metrics import f1_score\n",
        "f1 = f1_score(true_arr, pred_arr, average='micro')\n",
        "print(f1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QWJgEntFnSVH",
        "outputId": "b6040e42-2515-4dba-9c04-db2d2a5a7510"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.94\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"/content/drive/MyDrive/CS512_Assignment/assignment/news_train.txt\", 'r') as file:\n",
        "  count = 0\n",
        "  with open('/content/drive/MyDrive/CS512_Assignment/news_GPT_preds.txt', 'a') as write_file:\n",
        "    for line in file:\n",
        "      if count > 100:\n",
        "        break\n",
        "      if count > 90:\n",
        "        result = chatWithGPT_news(line)\n",
        "        print(f\"Iteration{count}: \", result)\n",
        "        write_file.write(result + '\\n')\n",
        "      count+=1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QgVeo3ckq37l",
        "outputId": "364cefa2-bb73-4b31-d8b0-aa8bac920237"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration91:  Technology\n",
            "Iteration92:  Technology\n",
            "Iteration93:  Sports\n",
            "Iteration94:  Sports\n",
            "Iteration95:  Sports\n",
            "Iteration96:  Politics\n",
            "Iteration97:  Business\n",
            "Iteration98:  Technology\n",
            "Iteration99:  Sports\n",
            "Iteration100:  Sports\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "true_labels = []\n",
        "pred_labels = []\n",
        "with open(\"/content/drive/MyDrive/CS512_Assignment/assignment/news_train_labels.txt\", 'r') as truth:\n",
        "  count = 0\n",
        "  for line in truth:\n",
        "    if count == 100: break\n",
        "    true_labels.append(int(line))\n",
        "with open(\"/content/drive/MyDrive/CS512_Assignment/news_GPT_preds.txt\", 'r') as pred:\n",
        "  count = 0\n",
        "  for line in pred:\n",
        "    if count == 100: break\n",
        "    if \"politics\" in line or \"Politics\" in line:\n",
        "      pred_labels.append(0)\n",
        "    elif \"sports\" in line or \"Sports\" in line:\n",
        "      pred_labels.append(1)\n",
        "    elif \"politics\" in line or \"Business\" in line:\n",
        "      pred_labels.append(2)\n",
        "    elif \"sports\" in line or \"Technology\" in line:\n",
        "      pred_labels.append(3)\n",
        "    else:\n",
        "      print(line)\n",
        "      pred_labels.append(5)\n",
        "true_arr = np.array(true_labels[:100])\n",
        "pred_arr = np.array(pred_labels[:100])\n",
        "\n",
        "from sklearn.metrics import f1_score\n",
        "f1 = f1_score(true_arr, pred_arr, average='micro')\n",
        "print(f1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5WoQNW_qrDV4",
        "outputId": "7f620ae7-6174-4d04-f985-a4b1de329b72"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.88\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Test Dataset with LLM"
      ],
      "metadata": {
        "id": "Oj4qRc2CPTPh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "with open(\"/content/drive/MyDrive/CS512_Assignment/assignment/news_test.txt\", 'r') as file:\n",
        "  count = 0\n",
        "  with open('/content/drive/MyDrive/CS512_Assignment/news_test_GPT_preds.txt', 'a') as write_file:\n",
        "    for line in file:\n",
        "      if count > 6674:\n",
        "        try:\n",
        "          result = chatWithGPT_news(line)\n",
        "        except:\n",
        "          time.sleep(2)\n",
        "          result = chatWithGPT_news(line)\n",
        "        print(f\"Iteration{count}: \", result)\n",
        "        write_file.write(result + '\\n')\n",
        "        #if count % 50 == 0:\n",
        "          #time.sleep(1)\n",
        "      count+=1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qw2JyuXcQBOI",
        "outputId": "b5cf6c85-4992-4ed0-cfe2-d04b09ef9ed1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration6675:  Politics\n",
            "Iteration6676:  Politics\n",
            "Iteration6677:  Sports\n",
            "Iteration6678:  Technology\n",
            "Iteration6679:  Sports\n",
            "Iteration6680:  Sports\n",
            "Iteration6681:  Business\n",
            "Iteration6682:  Sports\n",
            "Iteration6683:  Technology\n",
            "Iteration6684:  Politics\n",
            "Iteration6685:  Politics\n",
            "Iteration6686:  Business\n",
            "Iteration6687:  Business\n",
            "Iteration6688:  Sports\n",
            "Iteration6689:  Business\n",
            "Iteration6690:  Business\n",
            "Iteration6691:  Technology\n",
            "Iteration6692:  Politics\n",
            "Iteration6693:  Business\n",
            "Iteration6694:  Politics\n",
            "Iteration6695:  Technology\n",
            "Iteration6696:  Business\n",
            "Iteration6697:  Business\n",
            "Iteration6698:  Technology\n",
            "Iteration6699:  Technology\n",
            "Iteration6700:  Technology\n",
            "Iteration6701:  Business\n",
            "Iteration6702:  Politics\n",
            "Iteration6703:  Business\n",
            "Iteration6704:  Politics\n",
            "Iteration6705:  Sports\n",
            "Iteration6706:  Politics\n",
            "Iteration6707:  Technology\n",
            "Iteration6708:  Sports\n",
            "Iteration6709:  Politics\n",
            "Iteration6710:  Sports\n",
            "Iteration6711:  Sports\n",
            "Iteration6712:  Politics\n",
            "Iteration6713:  Business\n",
            "Iteration6714:  Sports\n",
            "Iteration6715:  Sports\n",
            "Iteration6716:  Business\n",
            "Iteration6717:  Politics\n",
            "Iteration6718:  Politics\n",
            "Iteration6719:  Business\n",
            "Iteration6720:  Sports\n",
            "Iteration6721:  Technology\n",
            "Iteration6722:  Technology\n",
            "Iteration6723:  Business\n",
            "Iteration6724:  Sports\n",
            "Iteration6725:  Business\n",
            "Iteration6726:  Politics\n",
            "Iteration6727:  Business\n",
            "Iteration6728:  Business\n",
            "Iteration6729:  Technology\n",
            "Iteration6730:  Technology\n",
            "Iteration6731:  Business\n",
            "Iteration6732:  Business\n",
            "Iteration6733:  Technology\n",
            "Iteration6734:  Politics\n",
            "Iteration6735:  Politics\n",
            "Iteration6736:  Technology\n",
            "Iteration6737:  Sports\n",
            "Iteration6738:  Business\n",
            "Iteration6739:  Sports\n",
            "Iteration6740:  Politics\n",
            "Iteration6741:  Sports\n",
            "Iteration6742:  Technology\n",
            "Iteration6743:  Technology\n",
            "Iteration6744:  Sports\n",
            "Iteration6745:  Sports\n",
            "Iteration6746:  Sports\n",
            "Iteration6747:  Politics\n",
            "Iteration6748:  Business\n",
            "Iteration6749:  Politics\n",
            "Iteration6750:  Sports\n",
            "Iteration6751:  Technology\n",
            "Iteration6752:  Sports\n",
            "Iteration6753:  Politics\n",
            "Iteration6754:  Business\n",
            "Iteration6755:  Technology\n",
            "Iteration6756:  Technology\n",
            "Iteration6757:  Business\n",
            "Iteration6758:  Technology\n",
            "Iteration6759:  Technology\n",
            "Iteration6760:  Business\n",
            "Iteration6761:  Sports\n",
            "Iteration6762:  Politics\n",
            "Iteration6763:  Technology\n",
            "Iteration6764:  Sports\n",
            "Iteration6765:  Business\n",
            "Iteration6766:  Technology\n",
            "Iteration6767:  Sports\n",
            "Iteration6768:  Sports\n",
            "Iteration6769:  Sports\n",
            "Iteration6770:  Technology\n",
            "Iteration6771:  Sports\n",
            "Iteration6772:  Sports\n",
            "Iteration6773:  Politics\n",
            "Iteration6774:  Technology\n",
            "Iteration6775:  Sports\n",
            "Iteration6776:  Business\n",
            "Iteration6777:  Technology\n",
            "Iteration6778:  Politics\n",
            "Iteration6779:  Politics\n",
            "Iteration6780:  Business\n",
            "Iteration6781:  Politics\n",
            "Iteration6782:  Sports\n",
            "Iteration6783:  Sports\n",
            "Iteration6784:  Business\n",
            "Iteration6785:  Politics\n",
            "Iteration6786:  Business\n",
            "Iteration6787:  Technology\n",
            "Iteration6788:  Politics\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open() as f1:\n",
        "  with open() as f2:\n",
        "    for line in f1:\n",
        "      if 'politics' in line or 'Politics' in line:\n",
        "        f2.write('0'+'\\n')\n",
        "      if 'sports' in line or 'Sports' in line:\n",
        "        f2.write('0'+'\\n')\n",
        "      if 'business' in line or 'Business' in line:\n",
        "        f2.write('0'+'\\n')\n",
        "      if 'technology' in line or 'Technology' in line:\n",
        "        f2.write('0'+'\\n')\n",
        "      else:\n",
        "        f2.write(line)"
      ],
      "metadata": {
        "id": "ZIMkidS748UI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "true_labels = []\n",
        "pred_labels = []\n",
        "with open(\"/content/drive/MyDrive/CS512_Assignment/assignment/news_train_labels.txt\", 'r') as truth:\n",
        "  count = 0\n",
        "  for line in truth:\n",
        "    if count == 100: break\n",
        "    true_labels.append(int(line))\n",
        "with open(\"/content/drive/MyDrive/CS512_Assignment/news_GPT_preds.txt\", 'r') as pred:\n",
        "  count = 0\n",
        "  for line in pred:\n",
        "    if count == 100: break\n",
        "    if \"politics\" in line or \"Politics\" in line:\n",
        "      pred_labels.append(0)\n",
        "    elif \"sports\" in line or \"Sports\" in line:\n",
        "      pred_labels.append(1)\n",
        "    elif \"politics\" in line or \"Business\" in line:\n",
        "      pred_labels.append(2)\n",
        "    elif \"sports\" in line or \"Technology\" in line:\n",
        "      pred_labels.append(3)\n",
        "    else:\n",
        "      print(line)\n",
        "      pred_labels.append(5)\n",
        "true_arr = np.array(true_labels[:100])\n",
        "pred_arr = np.array(pred_labels[:100])\n",
        "\n",
        "from sklearn.metrics import f1_score\n",
        "f1 = f1_score(true_arr, pred_arr, average='micro')\n",
        "print(f1)"
      ],
      "metadata": {
        "id": "w7PHGQT5QD8P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"/content/drive/MyDrive/CS512_Assignment/assignment/movies_test.txt\", 'r') as file:\n",
        "  count = 0\n",
        "  with open('/content/drive/MyDrive/CS512_Assignment/movies_test_GPT_preds.txt', 'w') as write_file:\n",
        "    for line in file:\n",
        "      if count > -1:\n",
        "        result = chatWithGPT_movies(line)\n",
        "        print(f\"Iteration{count}: \", result)\n",
        "        write_file.write(result + '\\n')\n",
        "      count+=1"
      ],
      "metadata": {
        "id": "MB41VIosSAKa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd '/content/drive/MyDrive/CS512_Assignment/WeSTClass/movies/'\n",
        "from gensim.models import Word2Vec\n",
        "model = Word2Vec.load('embedding')\n",
        "model.wv.save_word2vec_format('embeddings.txt', binary= False, write_header= False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3L7w481H50jR",
        "outputId": "d26ad670-4f7e-4d7c-849b-288b7c981429"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/CS512_Assignment/WeSTClass/movies\n"
          ]
        }
      ]
    }
  ]
}